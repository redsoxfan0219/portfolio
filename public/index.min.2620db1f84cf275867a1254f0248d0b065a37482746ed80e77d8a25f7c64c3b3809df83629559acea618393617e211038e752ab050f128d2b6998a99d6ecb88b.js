var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(e){const s=suggestions.classList.contains("d-none");if(s)return;const t=[...suggestions.querySelectorAll("a")];if(t.length===0)return;const n=t.indexOf(document.activeElement);if(e.key==="ArrowUp"){e.preventDefault();const s=n>0?n-1:0;t[s].focus()}else if(e.key==="ArrowDown"){e.preventDefault();const s=n+1<t.length?n+1:n;t[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description","lead"],index:["title","description","lead"]}});e.add({id:0,href:"/code/datadictionarycreator/",title:"Data Dictionary Creator",description:`My first Python package, adamic, is a simple solution for creating data dictionaries.
Installation # The package is available on PyPi and can be downloaded by running the following from the command line:
pip install adamic Use # After installing the package to your environment, import the package to your script, Jupyter notebook, or directly to the python3 command line.
from adamic import adamic To create your data dictionary, pass a Pandas dataframe to the create_data_dictionary() function:`,content:`My first Python package, adamic, is a simple solution for creating data dictionaries.
Installation # The package is available on PyPi and can be downloaded by running the following from the command line:
pip install adamic Use # After installing the package to your environment, import the package to your script, Jupyter notebook, or directly to the python3 command line.
from adamic import adamic To create your data dictionary, pass a Pandas dataframe to the create_data_dictionary() function:
adamic.create_data_dictionary(sample_df) The package will prompt you to supply definitions for each variable in the dataset. Hit Enter after supplying definition or if you want to define the variable later after the output file has been created.
Finally, you will be prompted to name your preferred file extension. .csv, .json, and .xlsx are the available options.
Source Code # The source code for adamic can be found at this GitHub repository.
`}),e.add({id:1,href:"/docs/api-documentation/",title:"API Documentation",description:"A sample of my API Documentation",content:`Coming soon!
`}),e.add({id:2,href:"/code/exploratorydataanalysis/",title:"Exploratory Data Analysis",description:"Samples of past EDA",content:`Double-click the README.md in the interactive window for more information.
For other examples of my data science projects, see this GitHub repository.
`}),e.add({id:3,href:"/docs/model-documentation/",title:"Model Documentation",description:"A sample of AI/ML Model Documentation",content:`Below is an example of AI-ML documentation that I\u0026rsquo;ve created in the past. The content of this text is designed meet expectations set in the Federal Reserve\u0026rsquo;s SR 11-7: Guidance on Model Risk Management.
Note: all proprietary details have been removed and the specific model attributes have been genericized.
Model Overview # Model Purpose # The purpose of Super Fantastic Superior Sample Weather Model (hereafter \u0026ldquo;Sample Weather\u0026rdquo; or \u0026ldquo;the model\u0026rdquo;) is to predict Fantastic Insurance Co.\u0026rsquo;s claim losses and loss frequencies following a catastrophic (\u0026ldquo;CAT\u0026rdquo;) wind-rain event, such as a hurricane.
Currently, when a severe weather event occurs, Fantastic Insurance actuaries produce loss and frequency estimates by reviewing past wind-rain events and applying a multiplication factor to the cost and frequencies of the previous event. This approach has proven ineffective at gauging losses and frequencies, especially when the present event differs dramatically in size and location from past wind-rain events.
The goal of the Sample Weather model is to use advanced analytics techniques to
predict losses and frequencies more accurately produce predictions more quickly than current practice allows, and continue producing predictions for the 30 days following a wind-rain event. Model Characteristics # Attribute Value Model Name Sample Weather Model Model ID No. 867-5309 Model Owner Tommy Tutone Model Client Jenny Jenny Production Date November 16, 1981 Deployment SageMaker Image ID 325389189899.dkr.ecr.us-east-2.amazonaws.com/sampleenvironment-sampleweathermodel-dev-deploy:latest Repository Details # Production Repository
https://github.com/redsoxfan0219/master/sampleweathermodel
Development Repository
https://github.com/redsoxfan0219/develop/sampleweathermodel
ETL Code
https://github.com/redsoxfan0219/ETL/sampleweathermodel
Final Training Data
s3://sample-environment/sampleweathermodel/eda/train/
Model Outputs # The model produces state-level loss and frequency estimates for the lower 48 states + the District of Columbia. The production code sends 98 model outputs to Teradata and the business partner\u0026rsquo;s S3 bucket. The output format is .csv.
Implementation Details # The model pipeline is triggered to run when a member of the catastrophe claims team registers a new catastrophic event ID within their Super Great Claims system. New IDs are typically established while storms are still forming off the Atlantic coast (i.e., before any related claims are filed).
When new claims are filed following a catastrophic event, the model runs each of the first 30 days after claim with the new CAT ID is filed. The model runs at 8:00 ET on the applicable days. The business SLA states that the model must upload predictions to the business S3 by 9:00 am ET on the applicable days.
Production Data # Data Input #1: New Claim Data # The first production data input consists of variables related to each of the new claims filed. Only claims that have the specific CAT ID flag are passed to the model. The following are the raw claim variables passed to the model:
Variable Definition Units claim_id 16-digit Unique identifier N/A req_dol Total amount filed amount in claim USD cocode US county code of claim location N/A loss_desc Text description of the loss N/A written Total written premium of the policy USD Data Input #2: Claimant History # The second data input includes the policyholder\u0026rsquo;s claim history.
Variable Definition Units claim_id 16-digit Unique identifier N/A pr_cl_fd Binary flag indicating prior claim filed on the policy in the last 12 months N/A pr_frd_nt Binary flag indicating prior fraud investigation of policyholder N/A inv_notes String of prior fraud investigator notes, if applicable; different incidents\u0026rsquo; notes separated by semicolon NA Rationale # Claim history is used to predict potential incidents of fraud. Historically, catastrophic events invite high levels of insurance fraud.
Cleaning, Transformations, and Feature Engineering # Claimant history is filtered to limit to the prior 6 months before the first filed CAT claim.
The fraud investigator notes are stemmed and lemmatized before undergoing TF-IDF counting. The static file (.csv)outlining the most important n-grams are sourced from the production S3 bucket.
Data Input #3: NOAA Weather Data # Weather data is sourced from the National Oceanic \u0026amp; Atmospheric Administration\u0026rsquo;s High-Resolution Rapid Refresh system.
Data Input #4: Prior Day Model Objects # Past Model Objects.
Methodology, Assumptions, and Parameters # The primary modeling framework is XGBoost, whose objective function is the sum of a training loss and a regularization metric. For more information on the model equation, see here.
For each of this project\u0026rsquo;s model objects, the modeling team tuned the following hyperparameters:
alpha (range: 0-1000) colsample_bytree (range: .1-.5) eta (learning rate)(range: .1-.5) lambda(range: 0-1000) The hyperparamters values vary for each of the 30 n-day models. The list of values can be found in the xgb_hp_values.csv file, available in the project S3 bucket.
Controls # Input Controls # The following are the input controls for the production model:
Incoming variables from each source are tested against dictionaries of expected variables. If there are meaningful discrepanies, the model execution fails and an error message is logged on AWS CloudWatch. Output Controls # For each model run, the following output controls are performed:
A log of each model run is saved to the production S3 bucket When the model outputs land in the business partner S3, a confirmation email is sent to the business partner and the lead modeler. `}),e.add({id:4,href:"/code/rentext/",title:"RenText",description:"RenText is my ongoing effort to use Python and advanced analytics to analyze Renaissance English texts. The data used in this project is from the Early English Books Online (EEBO) Text Creation Partnership.",content:`RenText is my ongoing effort to use Python and advanced analytics to analyze Renaissance English texts. The data used in this project is from the Early English Books Online (EEBO) Text Creation Partnership.
`}),e.add({id:5,href:"/docs/package-documentation/",title:"Package Documentation",description:"Coming soon!",content:`Coming soon!
`}),e.add({id:6,href:"/code/githubaction/",title:"GitHub Action",description:`Introduction to State Site Generators # The purpose of static site generators is two-fold:
To allow uses to write in a simple, lightweight markup language (typically Markdown or reStructuredText) To render that simple markup text into attractive, stylized HTML files The beauty of static site generators is that, after some initial setup, the process is reduced to a single command line statement, like the one I use to build the HTML for this site on my local machine:`,content:`Introduction to State Site Generators # The purpose of static site generators is two-fold:
To allow uses to write in a simple, lightweight markup language (typically Markdown or reStructuredText) To render that simple markup text into attractive, stylized HTML files The beauty of static site generators is that, after some initial setup, the process is reduced to a single command line statement, like the one I use to build the HTML for this site on my local machine:
sphinx-build -b html docs/ docs/_build As you can see, this statement only requires one to specify the source for the markup files, the destination of the outputs, and the format of those outputs.
Shortcoming of Static Site Generators # While convenient in many ways, static site generators are not perfect on their own.
For example, imagine you are building a documentation template that will be used by a large team. You want everyone to write in the Markdown or reStructuredText files you\u0026rsquo;ve built, and you want everyone to render those files into HTML when finished. Finally, you want them to host those HTML files as a GitHub Pages site that is attached to their code repository.
You\u0026rsquo;re not worried about the team writing in Markdown. But you don\u0026rsquo;t want to worry about everyone rendering the files into HTML, a process many of them may be unfamiliar with. What if they don\u0026rsquo;t remember to add the rendered files into the correct output destination? What if they forget to add that required -b flag?
As a technical writer, you want to support your team, but you also want to make your life easier, too. Every time you ask others to perform manual processes, you\u0026rsquo;re introducing a risk that others will come to you for help with those processes. How can you minimize the chances of that happening, allowing you to focus on more important tasks?
GitHub Actions, a CI/CD Tool # Generally, it\u0026rsquo;s a good idea to reduce the number of times users have to do the same action manually. Automation is almost always preferable. With the HTML build process, we have an excellent candidate for automation. But how do we do it?
The answer is GitHub Actions.
Introduced a few years back, GitHub Actions is a mechanism that can be used to automate workflows. It\u0026rsquo;s grown to become a key function for many teams\u0026rsquo; continuous integration, continuous deployment (CI/CD).
Under the hood, GitHub Actions works by receiving directions you write in a .yml file, which you post within your code repository under a ./.github/workflows subdirectory. This file provides instructions for instantiating and directing a virtual machine.
Using a GitHub Action in a Docs-as-Code Deployment # For our purposes, GitHub Actions is valuable because it can be used to automate the HTML build process and commit that content to your GitHub repository.
The best part? It\u0026rsquo;s super simple.
This page was built using a GitHub Action I put together in 20 minutes. You can find that .yml file here. I\u0026rsquo;ve also reproduced the contents of that file below.
Explanation of the GitHub Action .YML # Below are the contents of the file I wrote for my GitHub Action. I provide a walkthrough of the contents further down.
name: docs_pages_render on: push: branches: - main jobs: build_docs_job: runs-on: ubuntu-latest env: GITHUB_PAT: \$ {{ secrets.GITHUB_TOKEN }} steps: - name: Check out the main branch to the VM uses: actions/checkout@v2.3.4 - name: Set up Python in the VM uses: actions/setup-python@v2.2.1 with: python-version: 3.9 - name: Install dependencies run: | python -m pip install sphinx python -m pip install furo python -m pip install myst-parser python -m pip install sphinx_togglebutton python -m pip install sphinx-copybutton - name: Render HTML run: sphinx-build -b html docs/ docs/_build - name: Set up temporary repository and commit to HTML files run: | cd docs/_build git init touch .nojekyll git add . git config --local user.email \u0026#34;action@github.com\u0026#34; git config --local user.name \u0026#34;GitHub Action\u0026#34; git commit -m \u0026#39;Deploy rendered HTML\u0026#39; - name: Push rendered HTML to destination branch uses: ad-m/github-push-action@v0.6.0 with: github_token: \${{ secrets.GITHUB_TOKEN }} branch: gh-pages force: true directory: ./docs/_build Here are the steps performed as a result of the .yml file above:
When the repository\u0026rsquo;s event monitor detects a git push to the main branch, GitHub stands up a Linux (with Ubuntu distribution) virtual machine (VM).
The VM checks out the main branch of the repository.
The VM sets up Python v3.9.
The VM downloads (via pip) all necessary dependencies to build the HTML files.These dependencies are all outlined in the repository\u0026rsquo;s docs/conf.py file.
The VM renders the HTML from the reStructuredText and Markdown files.
The VM changes directory into the _build output directory.
The VM initiates a Git repository within the _build output directory.
The VM stages and commits the newly rendered HTML files, using a default git commit message.
Finally, the VM pushes the committed files back to my repository and deploys the HTML to the gh-pages branch.
And voilà! We have our rendered and deployed HTML. Click here to see the GitHub Pages site that I built using this CI/CD pipeline.
`}),e.add({id:7,href:"/docs/ai-ml-process-documentation/",title:"AI-ML Process Documentation",description:`When my Fortune 100 data science overhauled its software stack, shifting to cloud-based ML tools, leadership struggled with the mountain of information it needed to communicate to associates. Images, containers and Docker; S3, SageMaker, and Kubeflow; Informatica, Spark, and Databricks—where to begin? how should it be organized? how could process changes be tracked?
At first, leadership favored the simplest option: simply directing associates to publicly available documentation. The option had its appeal.`,content:`When my Fortune 100 data science overhauled its software stack, shifting to cloud-based ML tools, leadership struggled with the mountain of information it needed to communicate to associates. Images, containers and Docker; S3, SageMaker, and Kubeflow; Informatica, Spark, and Databricks—where to begin? how should it be organized? how could process changes be tracked?
At first, leadership favored the simplest option: simply directing associates to publicly available documentation. The option had its appeal. After all, most of the companies producing the parts of our stack already had polished docs available online. But early interviews with our new users revealed the limitations of this approach. Associates were overwhelmed by the options outlined in publicly available documentation; they didn\u0026rsquo;t know what features we could use in our enterprise-tailored version of the tools. And, of course, none of the public options documented things like internal access requests and the configuration of our custom Git repository template. Given the significant technical changes we were asking associates to undertake, employing a hodgepodge of documentation felt like it would be adding one more hurdle before our team.
I proposed an alternative: building a custom documentation site from the ground up using Markdown, Jekyll and GitHub Pages. This approach, essentially a docs as code workflow, had the added benefit of version controlling changes to the documentation with Git, which wasn\u0026rsquo;t available with the other option we had been using (SharePoint). My proposal was accepted, and, despite having little experience with my doc tool stack, I quickly got to work.
The finished product was a comprehensive, user-friendly, 80+ page documentation site. At the time of this writing, it remains one of the most regularly consulted technical documentation resources not only within my department but across the enterprise.
Below I include a few screenshots from the site to highlight its features. No proprietary information is featured in these images.
In this first image, you can see the site landing page. I was unsatisfied with the out-of-the-box landing page options available from my Jekyll theme, so I build this page from scratch using the Bootstrap framework. Other notable features here include:
Header font and color aligned via custom CSS to enterprise standards Buttons built with appropriate icons sourced from Font Awesome Link to the GitHub repository\u0026rsquo;s Issues page A functional search bar In this next image, you\u0026rsquo;ll see one of the second-level landing pages. A couple of things worth noting here:
On the left, a collapsible side navigation bar. This navigation bar remains fixed in place as a user scrolls down the page. I adjusted the theme\u0026rsquo;s Javascript to enable this functionality. A next page button. Because our development team explained these steps happening in a sequence, I thought it logical to connect one page to the next in a natural way. Next, you can see a typical content page from the site. A few features evident here include:
A functional top navigation bar, complete with dropdown functionality. It was important to me that the site maximize navigability, and the top navigation bar offers but one path for users to access the content they need. An \u0026ldquo;admonition\u0026rdquo; call-out box. A \u0026ldquo;Warning\u0026rdquo; is used here, but \u0026ldquo;Tip,\u0026rdquo; \u0026ldquo;Note\u0026rdquo;, and \u0026ldquo;Caution\u0026rdquo; call-outs can be found throughout the site, too. Images to match the text description. Because these instructions direct users to navigate to a feature of GitHub most of them were not familiar with, I thought including images would help them feel confident about the tasks they are being asked to perform. `}),e.add({id:8,href:"/code/fetchweatherscript/",title:"Fetch Current Weather",description:"I like knowing the weather as soon as I get up in the morning. However, I don\u0026rsquo;t always like poking my head outside to find out, and a simple Google search doesn\u0026rsquo;t provide me all the weather details I want to know directly in the search returns page. So I wrote a Python script that gives me exactly what I want. Because I\u0026rsquo;ll occasionally travel, I\u0026rsquo;ve set it up so that, rather than simply defaulting to my primary location, I can enter the name of the city for which I want the weather details.",content:`I like knowing the weather as soon as I get up in the morning. However, I don\u0026rsquo;t always like poking my head outside to find out, and a simple Google search doesn\u0026rsquo;t provide me all the weather details I want to know directly in the search returns page. So I wrote a Python script that gives me exactly what I want. Because I\u0026rsquo;ll occasionally travel, I\u0026rsquo;ve set it up so that, rather than simply defaulting to my primary location, I can enter the name of the city for which I want the weather details.
In most cases, the API I\u0026rsquo;m using defaults to reporting in units that aren\u0026rsquo;t intuitive to me (e.g., meters/second for wind speed). I\u0026rsquo;ve added a few basic functions to convert values to units I\u0026rsquo;m more familiar with. I also can never remember what the significance of the pressure readings are, so I\u0026rsquo;ve added a function that categorizes a pressure reading as either High, Medium, or Low.
For more information on the API this script uses to retrieve current weather data, please see this page. Keys for this API can be accessed after setting up an account. For security reasons, I\u0026rsquo;ve removed my API key from the script below.
import requests def get_weather(): def kelvin_to_fahrenheit(kelvin): fahrenheit = ((kelvin - 273.15) * (9/5) + 32) return fahrenheit def millibar_to_inchesHg(millibars): inHg = millibars / 33.864 return inHg def pressure_rating(inHg): if inHg \u0026gt; 30.20: pressure = \u0026#34;High Pressure\u0026#34; return pressure elif 29.80 \u0026lt;= inHg \u0026lt;= 30.20: pressure = \u0026#34;Medium Pressure\u0026#34; return pressure else: pressure = \u0026#34;Low Pressure\u0026#34; return pressure def metersPerSecond_to_MPH(metersPerSecond): mph = metersPerSecond * 2.237 return mph api_key = \u0026#39;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\u0026#39; base_url = \u0026#39;https://api.openweathermap.org/data/2.5/weather\u0026#39; city = input(\u0026#39;Enter city name: \u0026#39;) endpoint = f\u0026#39;{base_url}?q={city}\u0026amp;appid={api_key}\u0026#39; response = requests.get(endpoint) if response.status_code == 200: data = response.json() main = data[\u0026#39;weather\u0026#39;][0][\u0026#39;main\u0026#39;] description = data[\u0026#39;weather\u0026#39;][0][\u0026#39;description\u0026#39;] temp_kelvin = data[\u0026#39;main\u0026#39;][\u0026#39;temp\u0026#39;] feels_like_kelvin = data[\u0026#39;main\u0026#39;][\u0026#39;feels_like\u0026#39;] humidity = data[\u0026#39;main\u0026#39;][\u0026#39;humidity\u0026#39;] wind_metric = data[\u0026#39;wind\u0026#39;][\u0026#39;speed\u0026#39;] wind_speed = metersPerSecond_to_MPH(wind_metric) pressure_millibar = data[\u0026#39;main\u0026#39;][\u0026#39;pressure\u0026#39;] pressure = millibar_to_inchesHg(pressure_millibar) pressure_rating = pressure_rating(pressure) temp = kelvin_to_fahrenheit(temp_kelvin) feels_like = kelvin_to_fahrenheit(feels_like_kelvin) print(f\u0026#39;Primary Weather: {main}\u0026#39;) print(f\u0026#39;Description: {description}\u0026#39;) print(f\u0026#39;Humidity: {humidity}%\u0026#39;) print(f\u0026#39;Temperature: {temp:.1f}°F\u0026#39;) print(f\u0026#39;Feels like: {feels_like:.1f}°F\u0026#39;) print(f\u0026#39;Wind speed: {wind_speed:.1f} mph\u0026#39;) print(f\u0026#39;Pressure: {pressure:.2f} inchesHg: {pressure_rating}\u0026#39;) else: error_code = response.status_code print(f\u0026#39;There was a {error_code} error\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: get_weather() Here\u0026rsquo;s a sample output for my primary location, Columbus :
Weather: Clouds Description: overcast clouds Humidity: 46% Temperature: 44.7°F Feels like: 41.6°F Wind speed: 6.6 mph Pressure: 29.94 inchesHg: Medium Pressure `}),e.add({id:9,href:"/code/",title:"Code",description:"",content:""}),e.add({id:10,href:"/docs/",title:"Docs",description:"",content:""}),e.add({id:11,href:"/graphics/",title:"Graphics",description:"",content:""}),e.add({id:12,href:"/graphics/processflow/",title:"Process Flow",description:"A sample process flow map",content:`Process flow maps are vital to technical documentation. Whether used to describe business processes or data product architecture, they help everyone get on the same page by visualizing the abstract.
There are several software programs out there that can be used to create process flow maps. I\u0026rsquo;m most familiar with Microsoft Visio, a program in which I\u0026rsquo;ve developed custom templates and custom stencils. I am also familiar with Miro and FigJam, for which I\u0026rsquo;ve built basic plugins using TypeScript via the Figma Plugin API.
In my past work, I have used process flow maps primarily to visualize how AI/ML models are structured. With the help of my process flow maps, confused business partners and those outside the dev project team can begin to understand how a data product is functioning.
Below is a generic example of the kind of diagram I complete regularly in Visio. As you can see, this diagram visualizes how data comes into a model, how it is transformed, how it is processed, and what happens to outputs after they are produced.
`}),e.add({id:13,href:"/graphics/linkedinpost/",title:"LinkedIn Post",description:`Occasionally, I\u0026rsquo;ve been asked to produce graphics for social media posts on sites like LinkedIn. My go-to software for such requests is Adobe Illustrator.
One recent request was for a graphic celebrating my department\u0026rsquo;s cohort of incoming interns. I took pains to ensure consistency with enterprise-wide brand standards and recent section-wide guidance on social media engagement. Here\u0026rsquo;s what I came up with:`,content:`Occasionally, I\u0026rsquo;ve been asked to produce graphics for social media posts on sites like LinkedIn. My go-to software for such requests is Adobe Illustrator.
One recent request was for a graphic celebrating my department\u0026rsquo;s cohort of incoming interns. I took pains to ensure consistency with enterprise-wide brand standards and recent section-wide guidance on social media engagement. Here\u0026rsquo;s what I came up with:
`}),e.add({id:14,href:"/personal/",title:"Personal",description:"Before I started working in technical writing, I was an academic specializing in Renaissance English literature. I got my PhD at Ohio State. If you\u0026rsquo;re having trouble sleeping, you can read my dissertation. As you might expect from someone who studied literature, I like to read a lot. I still read Renaissance literature occasionally — Edmund Spenser and John Milton are still among my favorites. I\u0026rsquo;ve also come to really enjoy the work of Marcel Proust.",content:`Before I started working in technical writing, I was an academic specializing in Renaissance English literature. I got my PhD at Ohio State. If you\u0026rsquo;re having trouble sleeping, you can read my dissertation. As you might expect from someone who studied literature, I like to read a lot. I still read Renaissance literature occasionally — Edmund Spenser and John Milton are still among my favorites. I\u0026rsquo;ve also come to really enjoy the work of Marcel Proust. Beyond \u0026ldquo;highbrow literature\u0026rdquo;, I read widely in sociology, politics, natural history, music, and popular science. This, this, and this are a few books I read recently and enjoyed. During the pandemic, a friend introduced me to fly fishing, which has become a big part of my life. I try to spend at least a couple days a month on a river — even in the winter. Here\u0026rsquo;s a photo of me on the Au Sable River in Michigan in December. It was a very brisk 29°F with a wind chill of 6°! And, alas, no fish that day to offset the bitter cold.
I also like spending time with my family. My wife Nadia and I have been married for about five years. We recently got kayaks that we\u0026rsquo;ve been enjoying taking them out on the local waterways.
We also have a pug named Maple, who likes to think of herself as a guard dog. In truth, she\u0026rsquo;s better at sleeping than she is at scaring away the mailman and local joggers. Occasionally, she\u0026rsquo;ll take a break from her busy life to pose for a photo.
`}),e.add({id:15,href:"/",title:"Hi, I'm Ben Barksdale.",description:"Technical Writing, Documentation, Information Architecture",content:""}),e.add({id:16,href:"/resume/",title:"Resume",description:"The formal record of my past work and education",content:`Experience # Senior Documentation Engineer
Nationwide Mutual Insurance Company, Enterprise Analytics Office
2019—Present
Provide end-to-end documentation services for a variety of technical audiences, including data scientists, executives, internal auditors, and state regulators Lead change management and training efforts on topics such as CI/CD and MLOps Read and write sample code (Python, R, and SQL) for documentation purposes Collaborate with software engineers, data scientists, data engineers, and other subject matter experts within Agile framework Supervise the work of three junior documentation engineers Develop and curate technical content—e.g., sample code, training videos, instructional manuals— for enterprise data scientists on topics such as Docker, Kubernetes, Kubeflow, and AWS products (S3, SageMaker, CloudWatch) Edit whitepapers on technical subjects such as natural language processing, gradient boosting machines, exploratory data analysis best practices, etc. Use scripting languages (Python, R, PowerShell) to develop tools to automate tasks such as environment configuration and documentation processes Analyst Intern
US Government Accountability Office, Contracting and National Security Acquisitions
2018—2019
Audited US Department of Defense major defense acquisition programs Wrote and edited technical reports on military spending and acquisition practices Graduate Teaching Associate
The Ohio State University, Department of English
2016—2018
Taught college writing and literature classes to more than 300 students Facilitated content in-person and online Education # PhD, English — The Ohio State University (2020)
MA, English — The University of Alabama (2015)
BA, English — Western Michigan University (2013)
Skills \u0026amp; Technology # Technical Writing Developer Documentation Model Documentation API Documentation Package Documentation (Python, R) Docs as Code Editing Copyediting Proofreading Qualitative Research Model Risk Management Project Management Corporate Training Developing content for technical audiences Learning Development Instructional Design Model Development Lifecycle Python R SQL SharePoint Git/GitHub MS Office HTML/CSS Terminal/PowerShell `}),e.add({id:17,href:"/contributors/",title:"Contributors",description:"",content:""}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()