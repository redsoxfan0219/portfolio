var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(e){const s=suggestions.classList.contains("d-none");if(s)return;const t=[...suggestions.querySelectorAll("a")];if(t.length===0)return;const n=t.indexOf(document.activeElement);if(e.key==="ArrowUp"){e.preventDefault();const s=n>0?n-1:0;t[s].focus()}else if(e.key==="ArrowDown"){e.preventDefault();const s=n+1<t.length?n+1:n;t[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description","lead"],index:["title","description","lead"]}});e.add({id:0,href:"/code/",title:"Code",description:"",content:""}),e.add({id:1,href:"/code/datadictionarycreator/",title:"Data Dictionary Creator",description:`My first Python package, adamic, is a simple solution for creating data dictionaries.
Installation # The package is available on PyPi and can be downloaded by running the following from the command line:
pip install adamic Use # After installing the package to your environment, import the package to your script, Jupyter notebook, or directly to the python3 command line.
from adamic import adamic To create your data dictionary, pass a Pandas dataframe to the create_data_dictionary() function:`,content:`My first Python package, adamic, is a simple solution for creating data dictionaries.
Installation # The package is available on PyPi and can be downloaded by running the following from the command line:
pip install adamic Use # After installing the package to your environment, import the package to your script, Jupyter notebook, or directly to the python3 command line.
from adamic import adamic To create your data dictionary, pass a Pandas dataframe to the create_data_dictionary() function:
adamic.create_data_dictionary(sample_df) The package will prompt you to supply definitions for each variable in the dataset. Hit Enter after supplying definition or if you want to define the variable later after the output file has been created.
Finally, you will be prompted to name your preferred file extension. .csv, .json, and .xlsx are the available options.
Source Code # The source code for adamic can be found at this GitHub repository.
`}),e.add({id:2,href:"/docs/guides/",title:"Articles and Guides",description:"",content:""}),e.add({id:3,href:"/docs/overview/introduction/",title:"Introduction",description:"If you have questions about anything you see, I\u0026rsquo;d love to chat. Send me an email at ben.a.barksdale@gmail.com.",content:`If you have questions about anything you see, I\u0026rsquo;d love to chat. Send me an email at ben.a.barksdale@gmail.com.
`}),e.add({id:4,href:"/docs/sampleworkproducts/ai-ml-process-documentation/",title:"AI-ML Process Documentation",description:`Problem # When my Fortune 100 data science department overhauled its software stack, shifting to cloud-based ML tools, leadership struggled with the mountain of information it needed to communicate to associates. Images, containers, and Docker; S3, SageMaker, and Kubeflow; Informatica, Spark, and Databricks—where to begin? how should it be organized? how could process changes be tracked?
Proposals # At first, leadership favored the simplest option: simply directing associates to publicly available documentation.`,content:`Problem # When my Fortune 100 data science department overhauled its software stack, shifting to cloud-based ML tools, leadership struggled with the mountain of information it needed to communicate to associates. Images, containers, and Docker; S3, SageMaker, and Kubeflow; Informatica, Spark, and Databricks—where to begin? how should it be organized? how could process changes be tracked?
Proposals # At first, leadership favored the simplest option: simply directing associates to publicly available documentation. The option had its appeal. After all, most of the companies producing the parts of our stack already had passable, and in some cases quite good, documentation. But early interviews with our new users revealed the limitations of this approach. Associates were overwhelmed by the options outlined in publicly available documentation; they didn\u0026rsquo;t know what features we could use in our enterprise-tailored version of the tools. And, of course, none of the public options documented things like internal access requests and the configuration of our custom Git repository template. Given the significant technical changes we were asking associates to undertake, employing a hodgepodge of documentation felt like it would be adding one more hurdle before our team.
I proposed an alternative: building a custom documentation site from the ground up using Markdown, Jekyll, and GitHub Pages. This approach, essentially a docs as code workflow, had the added benefit of version controlling changes to the documentation with Git, which wasn\u0026rsquo;t available with the other option we had been using (SharePoint). My proposal was accepted, and, despite having little experience with my doc tool stack, I quickly got to work.
Solution: a Jeykll-Based Static Site # The finished product was a comprehensive, user-friendly, 80+ page documentation site. At the time of this writing, it remains one of the most regularly consulted technical documentation resources not only within my department but across the enterprise.
Below I include a few screenshots from the site to highlight its features. No proprietary information is featured in these images.
In this first image, you can see the site landing page. I was unsatisfied with the out-of-the-box landing page options available from my Jekyll theme, so I built this page from scratch using the Bootstrap framework. Other notable features here include:
Header font and color aligned via custom CSS to enterprise standards Buttons built with appropriate icons sourced from Font Awesome Link to the GitHub repository\u0026rsquo;s Issues page A functional search bar In this next image, you\u0026rsquo;ll see one of the second-level landing pages. A couple of things worth noting here:
On the left, a collapsible side navigation bar. This navigation bar remains fixed in place as a user scrolls down the page. I adjusted the Jekyll theme\u0026rsquo;s Javascript to enable this functionality. A next page button. Because our development team explained these steps happening in a sequence, I thought it logical to connect one page to the next in a natural way. Next, you can see a typical content page from the site. A few features evident here include:
A functional top navigation bar, complete with dropdown functionality. It was important to me that the site maximize navigability, and the top navigation bar offers but one path for users to access the content they need. An \u0026ldquo;admonition\u0026rdquo; call-out box. A \u0026ldquo;Warning\u0026rdquo; is used here, but \u0026ldquo;Tip,\u0026rdquo; \u0026ldquo;Note\u0026rdquo;, and \u0026ldquo;Caution\u0026rdquo; call-outs can be found throughout the site, too. Images to match the text description. Because these instructions direct users to navigate to a feature of GitHub most of them were not familiar with, I thought including images would help them feel confident about the tasks they are being asked to perform. `}),e.add({id:5,href:"/code/exploratorydataanalysis/",title:"Exploratory Data Analysis",description:"Samples of past EDA",content:`Double-click the README.md in the interactive window for more information.
For other examples of my data science projects, see this GitHub repository.
`}),e.add({id:6,href:"/docs/sampleworkproducts/",title:"Sample Work Products",description:"",content:""}),e.add({id:7,href:"/docs/sampleworkproducts/api-documentation/",title:"API Documentation",description:"A sample of my API Documentation",content:`Coming soon!
`}),e.add({id:8,href:"/docs/overview/",title:"Overview",description:"",content:""}),e.add({id:9,href:"/code/fetchweatherscript/",title:"Fetch Current Weather",description:"I like knowing the weather as soon as I get up in the morning. However, I don\u0026rsquo;t always like poking my head outside to find out, and a simple Google search doesn\u0026rsquo;t provide me all the weather details I want to know directly in the search returns page. So I wrote a Python script that gives me exactly what I want. Because I\u0026rsquo;ll occasionally travel, I\u0026rsquo;ve set it up so that, rather than simply defaulting to my primary location, I can enter the name of the city for which I want the weather details.",content:`I like knowing the weather as soon as I get up in the morning. However, I don\u0026rsquo;t always like poking my head outside to find out, and a simple Google search doesn\u0026rsquo;t provide me all the weather details I want to know directly in the search returns page. So I wrote a Python script that gives me exactly what I want. Because I\u0026rsquo;ll occasionally travel, I\u0026rsquo;ve set it up so that, rather than simply defaulting to my primary location, I can enter the name of the city for which I want the weather details.
In most cases, the API I\u0026rsquo;m using defaults to reporting in units that aren\u0026rsquo;t intuitive to me (e.g., meters/second for wind speed). I\u0026rsquo;ve added a few basic functions to convert values to units I\u0026rsquo;m more familiar with. I also can never remember what the significance of the pressure readings are, so I\u0026rsquo;ve added a function that categorizes a pressure reading as either High, Medium, or Low.
For more information on the API this script uses to retrieve current weather data, please see this page. Keys for this API can be accessed after setting up an account. For security reasons, I\u0026rsquo;ve removed my API key from the script below.
import requests def get_weather(): def kelvin_to_fahrenheit(kelvin): fahrenheit = ((kelvin - 273.15) * (9/5) + 32) return fahrenheit def millibar_to_inchesHg(millibars): inHg = millibars / 33.864 return inHg def pressure_rating(inHg): if inHg \u0026gt; 30.20: pressure = \u0026#34;High Pressure\u0026#34; return pressure elif 29.80 \u0026lt;= inHg \u0026lt;= 30.20: pressure = \u0026#34;Medium Pressure\u0026#34; return pressure else: pressure = \u0026#34;Low Pressure\u0026#34; return pressure def metersPerSecond_to_MPH(metersPerSecond): mph = metersPerSecond * 2.237 return mph api_key = \u0026#39;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\u0026#39; base_url = \u0026#39;https://api.openweathermap.org/data/2.5/weather\u0026#39; city = input(\u0026#39;Enter city name: \u0026#39;) endpoint = f\u0026#39;{base_url}?q={city}\u0026amp;appid={api_key}\u0026#39; response = requests.get(endpoint) if response.status_code == 200: data = response.json() main = data[\u0026#39;weather\u0026#39;][0][\u0026#39;main\u0026#39;] description = data[\u0026#39;weather\u0026#39;][0][\u0026#39;description\u0026#39;] temp_kelvin = data[\u0026#39;main\u0026#39;][\u0026#39;temp\u0026#39;] feels_like_kelvin = data[\u0026#39;main\u0026#39;][\u0026#39;feels_like\u0026#39;] humidity = data[\u0026#39;main\u0026#39;][\u0026#39;humidity\u0026#39;] wind_metric = data[\u0026#39;wind\u0026#39;][\u0026#39;speed\u0026#39;] wind_speed = metersPerSecond_to_MPH(wind_metric) pressure_millibar = data[\u0026#39;main\u0026#39;][\u0026#39;pressure\u0026#39;] pressure = millibar_to_inchesHg(pressure_millibar) pressure_rating = pressure_rating(pressure) temp = kelvin_to_fahrenheit(temp_kelvin) feels_like = kelvin_to_fahrenheit(feels_like_kelvin) print(f\u0026#39;Primary Weather: {main}\u0026#39;) print(f\u0026#39;Description: {description}\u0026#39;) print(f\u0026#39;Humidity: {humidity}%\u0026#39;) print(f\u0026#39;Temperature: {temp:.1f}°F\u0026#39;) print(f\u0026#39;Feels like: {feels_like:.1f}°F\u0026#39;) print(f\u0026#39;Wind speed: {wind_speed:.1f} mph\u0026#39;) print(f\u0026#39;Pressure: {pressure:.2f} inchesHg: {pressure_rating}\u0026#39;) else: error_code = response.status_code print(f\u0026#39;There was a {error_code} error\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: get_weather() Here\u0026rsquo;s a sample output for my primary location, Columbus :
Weather: Clouds Description: overcast clouds Humidity: 46% Temperature: 44.7°F Feels like: 41.6°F Wind speed: 6.6 mph Pressure: 29.94 inchesHg: Medium Pressure `}),e.add({id:10,href:"/docs/sampleworkproducts/model-documentation/",title:"Model Documentation",description:"A sample of AI/ML Model Documentation",content:`Model Overview # Model Purpose # The purpose of Super Fantastic Superior Sample Weather Model (hereafter \u0026ldquo;Sample Weather\u0026rdquo; or \u0026ldquo;the model\u0026rdquo;) is to predict Fantastic Insurance Co.\u0026rsquo;s claim losses and loss frequencies following a catastrophic (\u0026ldquo;CAT\u0026rdquo;) wind-rain event, such as a hurricane.
Currently, when a severe weather event occurs, Fantastic Insurance actuaries produce loss and frequency estimates by reviewing past wind-rain events and applying a multiplication factor to the cost and frequencies of the previous event. This approach has proven ineffective at gauging losses and frequencies, especially when the present event differs dramatically in size and location from past wind-rain events.
The goal of the Sample Weather model is to use advanced analytics techniques to
predict losses and frequencies more accurately produce predictions more quickly than current practice allows, and continue producing predictions for the 30 days following a wind-rain event. Model Characteristics # Attribute Value Model Name Sample Weather Model Model ID No. 867-5309 Model Owner Tommy Tutone Model Client Jenny Jenny Production Date November 16, 1981 Deployment SageMaker Image ID 325389189899.dkr.ecr.us-east-2.amazonaws.com/sampleenvironment-sampleweathermodel-dev-deploy:latest Repository Details # Production Repository
https://github.com/redsoxfan0219/master/sampleweathermodel
Development Repository
https://github.com/redsoxfan0219/develop/sampleweathermodel
ETL Code
https://github.com/redsoxfan0219/ETL/sampleweathermodel
Final Training Data
s3://sample-environment/sampleweathermodel/eda/train/
Below is an example of AI-ML documentation that I\u0026rsquo;ve created in the past. The content of this text is designed meet expectations set in the Federal Reserve\u0026rsquo;s SR 11-7: Guidance on Model Risk Management.
Note: all proprietary details have been removed and the specific model attributes have been genericized.
Model Outputs # The model produces state-level loss and frequency estimates for the lower 48 states + the District of Columbia. The production code sends 98 model outputs to Teradata and the business partner\u0026rsquo;s S3 bucket. The output format is .csv.
Implementation Details # The model pipeline is triggered to run when a member of the catastrophe claims team registers a new catastrophic event ID within their Super Great Claims system. New IDs are typically established while storms are still forming off the Atlantic coast (i.e., before any related claims are filed).
When new claims are filed following a catastrophic event, the model runs each of the first 30 days after claim with the new CAT ID is filed. The model runs at 8:00 ET on the applicable days. The business SLA states that the model must upload predictions to the business S3 by 9:00 am ET on the applicable days.
Production Data # Data Input #1: New Claim Data # The first production data input consists of variables related to each of the new claims filed. Only claims that have the specific CAT ID flag are passed to the model. The following are the raw claim variables passed to the model:
Variable Definition Units claim_id 16-digit Unique identifier N/A req_dol Total amount filed amount in claim USD cocode US county code of claim location N/A loss_desc Text description of the loss N/A written Total written premium of the policy USD Data Input #2: Claimant History # The second data input includes the policyholder\u0026rsquo;s claim history.
Variable Definition Units claim_id 16-digit Unique identifier N/A pr_cl_fd Binary flag indicating prior claim filed on the policy in the last 12 months N/A pr_frd_nt Binary flag indicating prior fraud investigation of policyholder N/A inv_notes String of prior fraud investigator notes, if applicable; different incidents\u0026rsquo; notes separated by semicolon NA Rationale # Claim history is used to predict potential incidents of fraud. Historically, catastrophic events invite high levels of insurance fraud.
Cleaning, Transformations, and Feature Engineering # Claimant history is filtered to limit to the prior 6 months before the first filed CAT claim.
The fraud investigator notes are stemmed and lemmatized before undergoing TF-IDF counting. The static file (.csv)outlining the most important n-grams are sourced from the production S3 bucket.
Data Input #3: NOAA Weather Data # Weather data is sourced from the National Oceanic \u0026amp; Atmospheric Administration\u0026rsquo;s High-Resolution Rapid Refresh system.
Data Input #4: Prior Day Model Objects # Past Model Objects.
Methodology, Assumptions, and Parameters # The primary modeling framework is XGBoost, whose objective function is the sum of a training loss and a regularization metric. For more information on the model equation, see here.
For each of this project\u0026rsquo;s model objects, the modeling team tuned the following hyperparameters:
alpha (range: 0-1000) colsample_bytree (range: .1-.5) eta (learning rate)(range: .1-.5) lambda(range: 0-1000) The hyperparamters values vary for each of the 30 n-day models. The list of values can be found in the xgb_hp_values.csv file, available in the project S3 bucket.
Controls # Input Controls # The following are the input controls for the production model:
Incoming variables from each source are tested against dictionaries of expected variables. If there are meaningful discrepanies, the model execution fails and an error message is logged on AWS CloudWatch. Output Controls # For each model run, the following output controls are performed:
A log of each model run is saved to the production S3 bucket When the model outputs land in the business partner S3, a confirmation email is sent to the business partner and the lead modeler. `}),e.add({id:11,href:"/code/rentext/",title:"RenText",description:`RenText is my ongoing effort to use computational tools to study Renaissance English texts, published roughly from 1470-1700.
Data Overview # The data used in this project is from the Text Creation Partnership.
The primary data currently consists of approximately 60,000 XML files, which can be accessed on the TCP\u0026rsquo;s Dropbox.
As described on this page, all of the 60K+ texts were hand-coded over the course of about 20 years. The exact nature of the EEBO source texts—digitized (often quite old and occasionally poor-quality) microfilm of original hard copies—made and continues to make optical character recognition infeasible.`,content:`RenText is my ongoing effort to use computational tools to study Renaissance English texts, published roughly from 1470-1700.
Data Overview # The data used in this project is from the Text Creation Partnership.
The primary data currently consists of approximately 60,000 XML files, which can be accessed on the TCP\u0026rsquo;s Dropbox.
As described on this page, all of the 60K+ texts were hand-coded over the course of about 20 years. The exact nature of the EEBO source texts—digitized (often quite old and occasionally poor-quality) microfilm of original hard copies—made and continues to make optical character recognition infeasible.
Data Updates # The latest updates from the TCP, available under \u0026ldquo;Phase II\u0026rdquo; on this page, indicate several thousand additional TCP texts are forthcoming. The page indicates this release was intended in 2020. However, as of October 2022, these updates have not been released.
Tools # I\u0026rsquo;m using Python for all my exploratory data analysis and cleaning. My primary libraries are pandas, re, xml.etree.ElementTree, and os.
For visualization, I am using graph modeling with neo4j.
Completed Work # Thus far, I have
Written a script that converts all .xml files to clean, human-readable .txt files Uploaded all .xml files to Amazon S3, enabling cloud-based computing Cleaned errors and anomalies in the titles\u0026rsquo; publication dates, creating a cleaned dates file that can be used for as a look-up table Written a script that returns basic metadata (author, title) for all titles for a given year Written a script that returns a random book\u0026rsquo;s information, including author, title, publication date, and sample paragraphs/lines of poetry Written a script to return instances of a word or phrase (i.e., an n-gram) for a given year Written a script to build a SQLite database with all primary metadata for all books in the TCP archive Plans # As of October 2022, my next steps include the following:
Optimize processing time in existing scripts by incorporating list comprehension and multiprocessing Publish full .txt files on S3 Building an API to expose values in SQLite database Building a website to publish returned API responses and provide download paths for full .txt files Building a Python package to simplify processing and analysis of TCP archive Building a graph database for visualizing the archive\u0026rsquo;s metadata Codebase and Supporting Resources # The code for this project can be found on this GitHub repository.
`}),e.add({id:12,href:"/docs/sampleworkproducts/package-documentation/",title:"Package Documentation",description:"Coming soon!",content:`Coming soon!
`}),e.add({id:13,href:"/docs/guides/introduction-to-the-command-line-for-technical-writers/",title:"Introduction to the Command Line for Technical Writers",description:"Coming soon!",content:`Coming soon!
`}),e.add({id:14,href:"/docs/guides/introduction-to-git-for-technical-writers/",title:"Introduction to Git for Technical Writers",description:`Increasingly, technical writing jobs require knowledge of something called \u0026ldquo;version control,\u0026rdquo; and by far the most popular version control system is a program called Git. It\u0026rsquo;s commonplace now for TW job posts simply list Git as a required skill, without explain explaining what it is or how it relates to TW.
This page provides an explanation of what Git is and how to use it. I emphasize topics of interests to technical writers, especially those who write or are interested in writing developer documentation.`,content:`Increasingly, technical writing jobs require knowledge of something called \u0026ldquo;version control,\u0026rdquo; and by far the most popular version control system is a program called Git. It\u0026rsquo;s commonplace now for TW job posts simply list Git as a required skill, without explain explaining what it is or how it relates to TW.
This page provides an explanation of what Git is and how to use it. I emphasize topics of interests to technical writers, especially those who write or are interested in writing developer documentation.
If you don\u0026rsquo;t want to read the explanation of what Git is, you can jump to the either the Git Instructions or Git Cheat Sheet.
What Is Version Control? # Version control is simply a way of tracking changes made to files stored within a directory on a computer. There\u0026rsquo;s a bit more to it than that is a good enough definition for now.
Why Should I Know Version Control? # Think about instances where you\u0026rsquo;ve collaborated on document with a peer using a program like Microsoft Word. Unless you were using a cloud-based version of Word in Microsoft Teams or SharePoint, you inevitably ran into situations where you had to maintain different file version. You probably exchanged several emails with attachments named Working Document_v2, Working Document_v2.1, or, God forbid, Working Document_Final. (And when that Final version proved to not really be \u0026ldquo;final,\u0026rdquo; you may have another version called Working Document_final_final. Where does it end?)
If you\u0026rsquo;ve experienced situations like the one described above, you\u0026rsquo;ve already experienced the reason why version control is necessary. It\u0026rsquo;s inevitable that the system described above will break down at some point. Someone will forget to download the latest version from their email, or someone will download the right version but forget to re-save with a different _v\u0026lt;\u0026gt; number.
The other major headache with this approach is not knowing what changes were made in each file version. Even if you devised a fool-proof method for the suffix-based versioning system, how can you know which changes were associated with which version? Maybe you remember broadly what each version covers. But to version a document properly, you would need a computer-like memory for the details or every version—every paragraph, every word, every comma—and you would need to be able to hold those multiple versions in your head and compare them line-by-line. This is, for all intents and purposes, impossible.
Therefore, we need some way of systematically tracking changes from one version to the next, some way to understand what changes are associated with each version of a file.
What we need is something called a version control system.
TL;DR You\u0026rsquo;ll give yourself headaches (and future you will hate present you) if you manually save multiple versions of a file.
What Is a Version Control System? # A version control system (VCS) is a set of computational tools that allow users to systematically track changes to file or set of files within a specific repository. A VCS also allows for users to \u0026ldquo;roll back\u0026rdquo; to an earlier version of their file(s) if they so choose.
What is Distributed Version Control? # There are two primary models of version control: centralized version control and distributed version control.
With centralized version control, all users on a team save their changes on their personal computers and save their changes to a centralized system of record in a server. (The proper term is \u0026ldquo;committing\u0026rdquo; a change; more on this later.)
With distributed version control, each developer on a team maintains a copy of that system of record on their personal computer. When developers want to join their code with others, they submit their system of record to an orchestration system that registers differences between the various systems of record. That orchestration system allows developers to reconcile differences and merge the various systems of record into a single system of record.
Having said all that, distributed version control is what I\u0026rsquo;ll be focused on in the rest of this article. While there are still teams out there that use centralized version control, distributed version control is far more popular because it\u0026rsquo;s faster and it better enables collaboration between team members.
TL;DR In all likelihood, you\u0026rsquo;ll only ever use distributed version control, so don\u0026rsquo;t worry about the difference between it and centralized version.
What is Git? # Git is a distributed VCS. It\u0026rsquo;s by far the most popular VCS in the world.
Unlike many programs technical writers may be familiar with (MS Word, MadCap Flare, etc.), Git is a command line tool. It is directed by textual commands entered in the command line, not by the point-and-click direction of a mouse.
Aside on the Command Line # For those technical writers that are unfamiliar with the command line, using it can be scary the first few times. You\u0026rsquo;re totally going to break your computer if you enter the wrong character, right? (Nah, not likely.)
Fear of the command line is completely understandable if you\u0026rsquo;ve never used it.But the fact is that the command line is, in many respects, superior to point-and-click direction once you get used to it. It\u0026rsquo;s oftentimes far quicker. You can do things with it that you can\u0026rsquo;t with your mouse. Using it brings you closer to the tools that developers use, helping to break down the barrier that can separate technical writers from their developer partners.
So don\u0026rsquo;t fear the command line. Embrace it.
Git and GitHub: Related But Distinct # You\u0026rsquo;ll sometimes hear Git used interchangeably with GitHub. That\u0026rsquo;s wrong. Git and GitHub are separate entities and do separate things.
Git, again, is a distributed version control system used for tracking the history and details of various file states. Git is run from the command line.
GitHub, by contrast, is a website and server used for storing Git repositories and their files. It also has a few other key functions that I won\u0026rsquo;t get into here.
What makes the distinction between Git and GitHub murkier is that, as their similar names might suggest, they are often used in tandem. Project developers will version control their code—and docs, as I\u0026rsquo;ll describe further below— on their local computer using Git. When they are ready, they will \u0026ldquo;push\u0026rdquo; their files and the accompanying Git history to GitHub, where the files and Git history can be accessed by their co-developers.
While I won\u0026rsquo;t describe them in detail here, you should know that GitHub isn\u0026rsquo;t the only Git storage and orchestration website out there. GitLab and Bitbucket are two other examples.
So, one of the distinctions between Git and GitHub is that you can have Git without GitHub, but you can\u0026rsquo;t have GitHub without Git.
TL;DR Git is a command line tool for versioning code and documentation. GitHub is a website for storing Git repositories. Git and GitHub are often used in combination.
Git Instructions # If you\u0026rsquo;re ready to start using Git, this section is for you.
Git Installation # For Mac # If you\u0026rsquo;re on a Mac, you will install Git via Homebrew. To do that,
Open your Terminal (the command line on Mac).
Enter /bin/bash -c \u0026quot;\$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026quot;
Wait for the downloads to finish. This can take some time.
When Homebrew has finished installing, enter brew install git.
For Windows # If you\u0026rsquo;re on a Windows machine,
Click here to download the Git Installer.
Open the installer where you\u0026rsquo;ve downloaded it (likely your Downloads folder).
Progress through the installer menu, accepting the defaults.
For Linux # If you\u0026rsquo;re on a Linux machine, the exact mechanism you\u0026rsquo;ll use will vary by your distribution.
Start by opening the Terminal. You might find this in your distribution by searching for \u0026ldquo;Shell.\u0026rdquo; Enter the command appropriate to your distribution. If you are using Fedora, RHEL, or CentOS, enter sudo dnf install git-all. If you are using Ubuntu or another Debian-based distribution, enter sudo apt install git-all Getting Started with Git # This section will cover the basic commands of Git. This section will cover 80% of what you\u0026rsquo;ll need to know to use Git for developer documentation.
Setting up GitHub # Further underscoring murky distinctions between Git and GitHub, we\u0026rsquo;ll start our Git journey by setting up a GitHub account. If you already have a GitHub account, skip to the next section. Note that if you\u0026rsquo;re using GitHub for your job, you\u0026rsquo;ll probably use a version of GitHub called GitHub Enterprise. If that\u0026rsquo;s the case, you\u0026rsquo;ll need to set up create a separate work account through GitHub Enterprise.
Navigate to github.com. Click \u0026lsquo;Sign up\u0026rsquo; in the upper-right. Follow the prompts to set up your account. Two Key Non-Git Commands # In just a moment, you\u0026rsquo;ll be using the command line to set up your local Git repository. But before you do that, you need to know a few non-Git commands.
I\u0026rsquo;ll keep things limited to the very basics here. If you\u0026rsquo;re interested in learning more about the command line, check out my Introduction to the Command Line for Technical Writers.
cd, Change Working Directory # When you open your command line application, the command line will default to working in some high-level directory (i.e., folder). It\u0026rsquo;s probably not the one you want to be working in, so you\u0026rsquo;ll need to change your directory. You\u0026rsquo;ll need to do this each time you open a new command line window.
When you open your command line, you\u0026rsquo;ll see a screen that looks like the following image. Your command line may look a little different, especially if you are using Windows\u0026rsquo; PowerShell. But functionally this should be the same.
The ~ benjaminmoran\$ indicates the working directory. You can think of this as communicating the computer\u0026rsquo;s present perspective: it currently sees the /Users/benjaminmoran folder and its contents and can perform actions on them.
However, most of the content you interact with is not stored in the default working directory that appears when you open a new terminal window (this is know as the \u0026ldquo;home directory\u0026rdquo;). If you want to perform actions on another directory\u0026rsquo;s contents, you can either tell the machine where those other contents are or you can move the working directory to that location and perform actions on them there. It\u0026rsquo;s generally easier to do the latter.
To move your working directory, you will enter: cd \u0026lt;new directory\u0026gt;, replacing \u0026lt;new directory\u0026gt; with the location that you want to work in. For example, I store my Git repositories in within my Documents folder. This image shows how I get there from my home directory:
I am now in my GitHub subfolder within Documents. Note that I didn\u0026rsquo;t need to run multiple cd commands. I knew my GitHub folder was in my Documents directory, so I was able to jump two directories down with one command.
Tab Completion # There\u0026rsquo;s a handy trick to make changing your directory even quicker: tab completion. When writing out a directory, for example, I can enter TAB after a couple of characters, and the terminal will fill in the directory. Here, I just typed Doc before hitting TAB, saving me a little bit of time. When you add up those little bits of time saved, though, you start to realize how much more efficient this is than navigating with your mouse.
Note that, with tab completion, your computer looks for unique values based on your initial input. If it encounters two file or directory names with the same root, the terminal will stop at the point that the two file names differ. It then expects you to clarify which file or directory you intend to use. For example, imagine I have two subdirectories in my present working directory, one called Document and one called Documents. If I type cd Doc and hit TAB, the terminal will fill in ument, stopping at cd Document, because this is where the two file names differ. The terminal expects me to either hit Enter to change directory into Document or enter an s before entering into the Documents directory.
ls, List Contents of the Working Directory # You\u0026rsquo;ve changed into your new directory. How do you know what\u0026rsquo;s there without looking at your File Explorer or Finder?
ls is here to help. ls lists the contents of your present working directory:
The only thing I want to point out here is that subdirectories and files are both printed when ls is entered. Subdirectories do not have a file extension; files do.
Setting Up a Git Repository # You can turn any ordinary folder into a Git repository, because Git repositories are just folders with a hidden .git file within them.
Cloning from Remote # There are two ways to set up a new Git repository:
Initiating a repository on GitHub and cloning it your local computer Initiating one on your local computer I typically do the former, so I\u0026rsquo;ll give those instructions first.
Sign in to GitHub. Click the green \u0026ldquo;New\u0026rdquo; button. On the new screen, enter a repository name. I recommend using underscores or hyphens to separate words in a repository name. Keep the accessibility option set to \u0026ldquo;Public.\u0026rdquo; Click the green \u0026ldquo;Create repository\u0026rdquo; button. On the new screen, copy your repository URL, which should appear within the \u0026ldquo;Quick Setup\u0026rdquo; pane. The URL will look something like this: https://github.com/redsoxfan0219/git-demo.git. We\u0026rsquo;re now going to add a dummy file so this repository is not empty. On the same screen, click the \u0026ldquo;README\u0026rdquo; link, which appears beneath the line with the Git URL. On the next screen, you\u0026rsquo;ll see some short text in the window pane. Scroll down and click the green \u0026ldquo;Commit new file\u0026rdquo; button\u0026quot;. Now, open a new command line window. On Windows, open PowerShell. On Mac, open Terminal. On Linux, open Terminal. cd to the directory where you want to store the local copy of your Git repository. Type git clone, then paste your the Git URL into the terminal, and hit Enter. You\u0026rsquo;ll see some stuff happen: Voilà! You\u0026rsquo;ve cloned your remote-native Git repository to your local computer. You can cd down into the repository and run an ls to see what\u0026rsquo;s there: As you can see, our local repository has the README.md file we created earlier.
Creating a Local Git Repository # We just cloned a remote copy of a Git repository to our local machine. If we don\u0026rsquo;t have a new project already set up, we can start from our local machine instead.
To start a new Git repository on your local machine,
Open a terminal window and cd to the location where you\u0026rsquo;d like to create a Git repository. Create a new directory using mkdir \u0026lt;name of your new folder\u0026gt;. cd into your new directory. Type git init -b main and hit Enter: We\u0026rsquo;ve turned our new folder git-test into a new Git repository, and we\u0026rsquo;ve given it a branch (via that -b flag) named main. More on branches in just a bit.
Staging Your Changes # After you\u0026rsquo;ve created a new Git repository via git init, you need to add some content to the repository before making your first \u0026ldquo;commit,\u0026rdquo; which essentially saves a snapshot of your Git repository at a given moment in time. Here, I\u0026rsquo;m going to use the Mac Terminal\u0026rsquo;s touch \u0026lt;file-name.extension\u0026gt; command to create a new dummy file (PowerShell users can enter the New-File \u0026lt;file-name.extension\u0026gt; command):
Our ls confirms the new file has been created.
Now, we\u0026rsquo;re (almost) ready to commit our work. Unlike in other programs you may be used to, Git \u0026ldquo;stages\u0026rdquo; changes before committing them. There\u0026rsquo;s a good reason for that, but I won\u0026rsquo;t get into it here.
To stage your changes, simply run
git add . The . is a shorthand which means \u0026ldquo;all\u0026rdquo;. Here, that means stage all files that have been changed. If you\u0026rsquo;d prefer, you can also stage files individually, like so:
git add README.md The terminal won\u0026rsquo;t print a confirmation after you stage your content. However, you can run the git status command to check if files have been staged. Files listed in green have been staged; files listed in red have not. Here are the results of running git status before and staging our file:
Committing Your Contents # Finally, we\u0026rsquo;re now ready to commit our changes. To commit your changes, you will run git commit -m \u0026quot;some message\u0026quot;, replacing \u0026ldquo;some message\u0026rdquo; with a meaningful description of the changes reflected in this commit. Don\u0026rsquo;t skimp on the message! You may need to use find this commit later, and the message will help you know what this commit captures.
After you hit Enter, Git will display a summary of your committed changes.
To see our most recent commits, you can run git log to see your most recent changes, beginning with your latest commit. I\u0026rsquo;ve made a few additional commits here for demonstration purposes.
That long string of letters and numbers is the \u0026ldquo;commit hash.\u0026rdquo; Each commit hash is unique. It\u0026rsquo;s what we\u0026rsquo;ll use if we ever need to \u0026ldquo;roll back\u0026rdquo; our contents to match the state of the repository reflected in the commit hash.
Moving Your Local Git Repository to GitHub # If you\u0026rsquo;ve initiated your Git repository on your local machine and you\u0026rsquo;ve made at least one commit, you\u0026rsquo;re ready to connect your local Git repository to a remote Git storage system like GitHub.
To do so,
Navigate to GitHub and sign-in to your account if necessary.
Click the + in the upper-right corner, and select \u0026ldquo;New repository\u0026rdquo; from the dropdown.
Add a repository name and click the green \u0026ldquo;Create repository\u0026rdquo; button. While not strictly required, it is a very good idea to match this name to the name of your local Git repository.
On the next page, copy the Git URL
Switch over to your terminal window.
If necessary, cd into your local Git repository.
Enter git remote add origin, paste your GitHub URL, and hit Enter.
Verify the connection by entering git remote -v. Both the fetch and push URLs should match the URL you just entered.
Git Branches # The last major thing I\u0026rsquo;ll discuss in this introduction is what branching is and how it affects how you\u0026rsquo;ll use Git.
Branches are a feature of distributed version control that allow you to store multiple versions of the same Git repository. This allows each developer to work on their own branch before merging their contributions to another branch designated by a team as the primary branch. Nor are you restricted to one branch per person. You might want to perform your testing on a test branch, draft your documentation on a docs-draft branch, and your save your polished code on a feature branch. The names can be whatever they want. However, a developer team may have a \u0026ldquo;branching strategy\u0026rdquo; that guides how branches are named and how they interact with one another.
Adding a New Branch # A few steps ago, we created our first branch when we initiated a Git repository. The command we used was git init -b main. You will use a different Git command for creating subsequent new branches.
There are a few ways of creating new branches, but the one I like most is git checkout -b \u0026lt;branch-name\u0026gt;. This step combines two steps: it creates a new branch and changes your working branch to the new branch. You can confirm which branch you are on my entering git branch. The following image demonstrates what git branch shows before and after running git checkout -b \u0026lt;branch-name\u0026gt;.
While Git prints a message after running git checkout, you should run git branch regularly to ensure you\u0026rsquo;re working on your intended branch.
Switching to an Existing Local Branch # While git checkout -b \u0026lt;branch\u0026gt; is nice for switching to a new branch, sometimes you\u0026rsquo;ll need to switch to a different branch you previously created. This one is pretty straightforward: use \`git switch .
If you can\u0026rsquo;t remember the existing branch name, you can always run a git branch to double-check the existing local branches.
Pushing a Local Branch to Remote # It\u0026rsquo;s a good practice to send your local branch changes to the remote at least periodically. To do so, after staging and committing your changes, all you need to do is run git push.
Getting All Remote Branches on Your Local Repository # Especially when working with collaborators, you will find that your remote repository eventually contains more branches than your local repository.
Getting all the remote branches and their updates is a bit complicated. You can copy the commands below:
git branch -r | grep -v \u0026#39;\\-\u0026gt;\u0026#39; | sed \u0026#34;s,\\x1B\\[[0-9;]*[a-zA-Z],,g\u0026#34; | while read remote; do git branch --track \u0026#34;\${remote#origin/}\u0026#34; \u0026#34;\$remote\u0026#34;; done git fetch --all git pull --all Git Cheat Sheet # git add \u0026lt;. or filename.extension\u0026gt;
Stages your change(s) prior to a commit. git commit -m \u0026lt;\u0026quot;commit message\u0026quot;\u0026gt;
Commits your changes. -m flag + \u0026quot;commit message\u0026quot; used to explain what the commit involves. git remote add origin \u0026lt;GitHub URL\u0026gt;
Establishes remote Git repository connection for repositories created locally. git remote -v
Prints the fetch and push URLs of the remote Git repository (e.g., GitHub). git fetch
Retrieves the latest metadata from the remote branch. Does NOT change the codebase on the local Git repository. git pull
Retrieves the latest metadata from the remote branch AND updates the codebase in the local working branch. git push
When a connection to remote has been established and at least one new commit has been made, transmits commit contents and metadata to remote Git repository. git checkout -b \u0026lt;branch-name\u0026gt;
Creates a new branch and switches the working branch to it. git switch \u0026lt;branch-name\u0026gt;
Changes the working branch. git branch
Prints all local branches and identifies the working branch with a *. git branch -vv
Prints all local branches alongside each branch\u0026rsquo;s most recent commit details (shortform commit hash and commit message). Identifies the working branch with a *. git branch -a
Prints all local AND remote branches. Identifies the local working branch with a *. git log
Prints details for the last three commits, including the commit hash, commit author, and the timestamp of the commit. `}),e.add({id:15,href:"/docs/guides/introduction-to-github-for-technical-writers/",title:"Introduction to GitHub for Technical Writers",description:"",content:""}),e.add({id:16,href:"/docs/guides/github-actions/",title:"GitHub Actions",description:`Introduction to State Site Generators # The purpose of static site generators is two-fold:
To allow uses to write in a simple, lightweight markup language (typically Markdown or reStructuredText) To render that simple markup text into attractive, stylized HTML files The beauty of static site generators is that, after some initial setup, the process is reduced to a single command line statement, like the one I use to build the HTML for this site on my local machine:`,content:`Introduction to State Site Generators # The purpose of static site generators is two-fold:
To allow uses to write in a simple, lightweight markup language (typically Markdown or reStructuredText) To render that simple markup text into attractive, stylized HTML files The beauty of static site generators is that, after some initial setup, the process is reduced to a single command line statement, like the one I use to build the HTML for this site on my local machine:
sphinx-build -b html docs/ docs/_build As you can see, this statement only requires one to specify the source for the markup files, the destination of the outputs, and the format of those outputs.
Shortcoming of Static Site Generators # While convenient in many ways, static site generators are not perfect on their own.
For example, imagine you are building a documentation template that will be used by a large team. You want everyone to write in the Markdown or reStructuredText files you\u0026rsquo;ve built, and you want everyone to render those files into HTML when finished. Finally, you want them to host those HTML files as a GitHub Pages site that is attached to their code repository.
You\u0026rsquo;re not worried about the team writing in Markdown. But you don\u0026rsquo;t want to worry about everyone rendering the files into HTML, a process many of them may be unfamiliar with. What if they don\u0026rsquo;t remember to add the rendered files into the correct output destination? What if they forget to add that required -b flag?
As a technical writer, you want to support your team, but you also want to make your life easier, too. Every time you ask others to perform manual processes, you\u0026rsquo;re introducing a risk that others will come to you for help with those processes. How can you minimize the chances of that happening, allowing you to focus on more important tasks?
GitHub Actions, a CI/CD Tool # Generally, it\u0026rsquo;s a good idea to reduce the number of times users have to do the same action manually. Automation is almost always preferable. With the HTML build process, we have an excellent candidate for automation. But how do we do it?
The answer is GitHub Actions.
Introduced a few years back, GitHub Actions is a mechanism that can be used to automate workflows. It\u0026rsquo;s grown to become a key function for many teams\u0026rsquo; continuous integration, continuous deployment (CI/CD).
Under the hood, GitHub Actions works by receiving directions you write in a .yml file, which you post within your code repository under a ./.github/workflows subdirectory. This file provides instructions for instantiating and directing a virtual machine.
Using a GitHub Action in a Docs-as-Code Deployment # For our purposes, GitHub Actions is valuable because it can be used to automate the HTML build process and commit that content to your GitHub repository.
The best part? It\u0026rsquo;s super simple.
This page was built using a GitHub Action I put together in 20 minutes. You can find that .yml file here. I\u0026rsquo;ve also reproduced the contents of that file below.
Explanation of the GitHub Action .YML # Below are the contents of the file I wrote for my GitHub Action. I provide a walkthrough of the contents further down.
name: docs_pages_render on: push: branches: - main jobs: build_docs_job: runs-on: ubuntu-latest env: GITHUB_PAT: \$ {{ secrets.GITHUB_TOKEN }} steps: - name: Check out the main branch to the VM uses: actions/checkout@v2.3.4 - name: Set up Python in the VM uses: actions/setup-python@v2.2.1 with: python-version: 3.9 - name: Install dependencies run: | python -m pip install sphinx python -m pip install furo python -m pip install myst-parser python -m pip install sphinx_togglebutton python -m pip install sphinx-copybutton - name: Render HTML run: sphinx-build -b html docs/ docs/_build - name: Set up temporary repository and commit to HTML files run: | cd docs/_build git init touch .nojekyll git add . git config --local user.email \u0026#34;action@github.com\u0026#34; git config --local user.name \u0026#34;GitHub Action\u0026#34; git commit -m \u0026#39;Deploy rendered HTML\u0026#39; - name: Push rendered HTML to destination branch uses: ad-m/github-push-action@v0.6.0 with: github_token: \${{ secrets.GITHUB_TOKEN }} branch: gh-pages force: true directory: ./docs/_build Here are the steps performed as a result of the .yml file above:
When the repository\u0026rsquo;s event monitor detects a git push to the main branch, GitHub stands up a Linux (with Ubuntu distribution) virtual machine (VM).
The VM checks out the main branch of the repository.
The VM sets up Python v3.9.
The VM downloads (via pip) all necessary dependencies to build the HTML files.These dependencies are all outlined in the repository\u0026rsquo;s docs/conf.py file.
The VM renders the HTML from the reStructuredText and Markdown files.
The VM changes directory into the _build output directory.
The VM initiates a Git repository within the _build output directory.
The VM stages and commits the newly rendered HTML files, using a default git commit message.
Finally, the VM pushes the committed files back to my repository and deploys the HTML to the gh-pages branch.
And voilà! We have our rendered and deployed HTML. Click here to see the GitHub Pages site that I built using this CI/CD pipeline.
`}),e.add({id:17,href:"/docs/",title:"Docs",description:"",content:""}),e.add({id:18,href:"/graphics/",title:"Graphics",description:"",content:""}),e.add({id:19,href:"/graphics/processflow/",title:"Process Flow",description:"A sample process flow map",content:`Process flow maps are vital to technical documentation. Whether used to describe business processes or data product architecture, they help everyone get on the same page by visualizing the abstract.
There are several software programs out there that can be used to create process flow maps. I\u0026rsquo;m most familiar with Microsoft Visio, a program in which I\u0026rsquo;ve developed custom templates and custom stencils. I am also familiar with Miro and FigJam, for which I\u0026rsquo;ve built basic plugins using TypeScript via the Figma Plugin API.
In my past work, I have used process flow maps primarily to visualize how AI/ML models are structured. With the help of my process flow maps, confused business partners and those outside the dev project team can begin to understand how a data product is functioning.
Below is a generic example of the kind of diagram I complete regularly in Visio. As you can see, this diagram visualizes how data comes into a model, how it is transformed, how it is processed, and what happens to outputs after they are produced.
`}),e.add({id:20,href:"/graphics/linkedinpost/",title:"LinkedIn Post",description:`Occasionally, I\u0026rsquo;ve been asked to produce graphics for social media posts on sites like LinkedIn. My go-to software for such requests is Adobe Illustrator.
One recent request was for a graphic celebrating my department\u0026rsquo;s cohort of incoming interns. I took pains to ensure consistency with enterprise-wide brand standards and recent section-wide guidance on social media engagement. Here\u0026rsquo;s what I came up with:`,content:`Occasionally, I\u0026rsquo;ve been asked to produce graphics for social media posts on sites like LinkedIn. My go-to software for such requests is Adobe Illustrator.
One recent request was for a graphic celebrating my department\u0026rsquo;s cohort of incoming interns. I took pains to ensure consistency with enterprise-wide brand standards and recent section-wide guidance on social media engagement. Here\u0026rsquo;s what I came up with:
`}),e.add({id:21,href:"/personal/",title:"Personal",description:"Before I started working in technical writing, I was an academic specializing in Renaissance English literature. I got my PhD at Ohio State. If you\u0026rsquo;re having trouble sleeping, you can read my dissertation. As you might expect from someone who studied literature, I like to read a lot. I still read Renaissance literature occasionally — Edmund Spenser and John Milton are still among my favorites. I\u0026rsquo;ve also come to really enjoy the work of Marcel Proust.",content:`Before I started working in technical writing, I was an academic specializing in Renaissance English literature. I got my PhD at Ohio State. If you\u0026rsquo;re having trouble sleeping, you can read my dissertation. As you might expect from someone who studied literature, I like to read a lot. I still read Renaissance literature occasionally — Edmund Spenser and John Milton are still among my favorites. I\u0026rsquo;ve also come to really enjoy the work of Marcel Proust. Beyond \u0026ldquo;highbrow literature\u0026rdquo;, I read widely in sociology, politics, natural history, music, and popular science. This, this, and this are a few books I read recently and enjoyed. During the pandemic, a friend introduced me to fly fishing, which has become a big part of my life. I try to spend at least a couple days a month on a river — even in the winter. Here\u0026rsquo;s a photo of me on the Au Sable River in Michigan in December. It was a very brisk 29°F with a wind chill of 6°! And, alas, no fish that day to offset the bitter cold.
I also like spending time with my family. My wife Nadia and I have been married for about five years. We recently bought kayaks and we\u0026rsquo;ve been enjoying taking them out on the local waterways.
We also have a pug named Maple, who likes to think of herself as a guard dog. In truth, she\u0026rsquo;s better at sleeping than she is at scaring away the mailman and local joggers. Occasionally, she\u0026rsquo;ll take a break from her busy life to pose for a photo.
`}),e.add({id:22,href:"/",title:"Hi, I'm Ben Barksdale.",description:"Technical Writing, Documentation, Information Architecture",content:""}),e.add({id:23,href:"/resume/",title:"Resume",description:"The formal record of my past work and education",content:`Experience # Senior Documentation Engineer
Nationwide Mutual Insurance Company, Enterprise Analytics Office
2019—Present
Provide end-to-end documentation services for a variety of technical audiences, including data scientists, executives, internal auditors, and state regulators Lead change management and training efforts on topics such as CI/CD and MLOps Read and write sample code (Python, R, and SQL) for documentation purposes Collaborate with software engineers, data scientists, data engineers, and other subject matter experts within Agile framework Supervise the work of three junior documentation engineers Develop and curate technical content—e.g., sample code, training videos, instructional manuals— for enterprise data scientists on topics such as Docker, Kubernetes, Kubeflow, and AWS products (S3, SageMaker, CloudWatch) Edit whitepapers on technical subjects such as natural language processing, gradient boosting machines, exploratory data analysis best practices, etc. Use scripting languages (Python, R, PowerShell) to develop tools to automate tasks such as environment configuration and documentation processes Analyst Intern
US Government Accountability Office, Contracting and National Security Acquisitions
2018—2019
Audited US Department of Defense major defense acquisition programs Wrote and edited technical reports on military spending and acquisition practices Graduate Teaching Associate
The Ohio State University, Department of English
2016—2018
Taught college writing and literature classes to more than 300 students Facilitated content in-person and online Education # PhD, English — The Ohio State University (2020)
MA, English — The University of Alabama (2015)
BA, English — Western Michigan University (2013)
Skills \u0026amp; Technology # Advanced Proficiency # Writing
Technical Writing Developer Documentation Model Documentation Editing Copyediting Proofreading Technology
GitHub SharePoint MS Office Markdown Other
Developing content for technical audiences Process Management Model Risk Management Project Management Learning Development Intermediate Proficiency # Writing
API Documentation Package / SDK Documentation Docs as Code Technology
Python reStructuredText R HTML/CSS Bash/PowerShell/Command Line Git Illustrator Other
Corporate Training Instructional Design Model Development Lifecycle Basic Proficiency # Technology
SQL Go Julia GIMP Other
Qualitative Research `}),e.add({id:24,href:"/contributors/",title:"Contributors",description:"",content:""}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()