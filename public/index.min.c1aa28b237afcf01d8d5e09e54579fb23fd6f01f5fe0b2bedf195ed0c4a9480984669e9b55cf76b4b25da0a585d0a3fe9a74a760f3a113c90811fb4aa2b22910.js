var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(e){const s=suggestions.classList.contains("d-none");if(s)return;const t=[...suggestions.querySelectorAll("a")];if(t.length===0)return;const n=t.indexOf(document.activeElement);if(e.key==="ArrowUp"){e.preventDefault();const s=n>0?n-1:0;t[s].focus()}else if(e.key==="ArrowDown"){e.preventDefault();const s=n+1<t.length?n+1:n;t[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description","lead"],index:["title","description","lead"]}});e.add({id:0,href:"/code/",title:"Code",description:"",content:""}),e.add({id:1,href:"/code/datadictionarycreator/",title:"Data Dictionary Creator",description:`My first Python package, adamic, is a simple solution for creating data dictionaries.
Installation # The package is available on PyPi and can be downloaded by running the following from the command line:
pip install adamic Use # After installing the package to your environment, import the package to your script, Jupyter notebook, or directly to the python3 command line.
from adamic import adamic To create your data dictionary, pass a Pandas dataframe to the create_data_dictionary() function:`,content:`My first Python package, adamic, is a simple solution for creating data dictionaries.
Installation # The package is available on PyPi and can be downloaded by running the following from the command line:
pip install adamic Use # After installing the package to your environment, import the package to your script, Jupyter notebook, or directly to the python3 command line.
from adamic import adamic To create your data dictionary, pass a Pandas dataframe to the create_data_dictionary() function:
adamic.create_data_dictionary(sample_df) The package will prompt you to supply definitions for each variable in the dataset. Hit Enter after supplying definition or if you want to define the variable later after the output file has been created.
Finally, you will be prompted to name your preferred file extension. .csv, .json, and .xlsx are the available options.
Source Code # The source code for adamic can be found at this GitHub repository.
`}),e.add({id:2,href:"/docs/guides/",title:"Articles and Guides",description:"",content:""}),e.add({id:3,href:"/docs/overview/introduction/",title:"Introduction",description:"If you have questions about anything you see, I\u0026rsquo;d love to chat. Send me an email at ben.a.barksdale@gmail.com.",content:`If you have questions about anything you see, I\u0026rsquo;d love to chat. Send me an email at ben.a.barksdale@gmail.com.
`}),e.add({id:4,href:"/docs/sampleworkproducts/ai-ml-process-documentation/",title:"AI-ML Process Documentation",description:`Problem # When my Fortune 100 data science department overhauled its software stack, leadership struggled with the mountain of information it needed to communicate to associates. Images, containers, and Docker; S3, SageMaker, and Kubeflow; Informatica, Spark, and Databricks—where to begin? how should it be organized? how could process changes be tracked?
Proposals # At first, leadership favored the simplest option: directing associates to publicly available documentation. The option had its appeal.`,content:`Problem # When my Fortune 100 data science department overhauled its software stack, leadership struggled with the mountain of information it needed to communicate to associates. Images, containers, and Docker; S3, SageMaker, and Kubeflow; Informatica, Spark, and Databricks—where to begin? how should it be organized? how could process changes be tracked?
Proposals # At first, leadership favored the simplest option: directing associates to publicly available documentation. The option had its appeal. After all, most of the companies producing the parts of our stack already had passable, and in some cases quite good, documentation. But early interviews with our new users revealed the limitations of this approach. Associates were overwhelmed by the options outlined in publicly available documentation; they didn\u0026rsquo;t know what features we could use in our enterprise-tailored version of the tools. And, of course, none of the public options documented things like internal access requests and the configuration of our custom Git repository template. Given the significant technical changes we were asking associates to undertake, employing a hodgepodge of documentation felt like it would be adding one more hurdle to the learning process.
I proposed an alternative: building a custom documentation site from the ground up using Markdown, Jekyll, and GitHub Pages. This approach, essentially a docs as code workflow, had the added benefit of version controlling changes to the documentation with Git, which wasn\u0026rsquo;t available with the other option we had been using (SharePoint). My proposal was accepted, and, despite having little experience with my doc tool stack, I quickly got to work.
Solution: a Jeykll-Based Static Site # The result of my work was a comprehensive, user-friendly, 80+ page documentation site. At the time of this writing, it remains one of the most regularly consulted technical documentation resources not only within my department but across the enterprise.
Below I include a few screenshots from the site to highlight its features. No proprietary information is featured in these images.
In this first image, you can see the site landing page. I was unsatisfied with the out-of-the-box landing page options available from my Jekyll theme, so I built this page from scratch using the Bootstrap framework. Other notable features here include:
Header font and color aligned to enterprise standards. I modified the CSS to accomplish this. Buttons built with appropriate icons sourced from Font Awesome Link to the GitHub repository\u0026rsquo;s Issues page A functional search bar In the next image, you can see one of the second-level landing pages. There are a couple of things worth noting here:
On the left, a collapsible side navigation bar. This navigation bar remains fixed in place as a user scrolls down the page. I adjusted the Jekyll theme\u0026rsquo;s Javascript to enable this functionality. A next page button. Because our development team explained these steps happening in a sequence, I thought it logical to connect one page to the next naturally. Next, you can see a typical content page from the site. A few features evident here include:
A functional top navigation bar, complete with dropdown functionality. It was important to me that the site maximize navigability, and the top navigation bar offers but one path for users to access the content they need. An \u0026ldquo;admonition\u0026rdquo; call-out box. A \u0026ldquo;Warning\u0026rdquo; is used here, but \u0026ldquo;Tip,\u0026rdquo; \u0026ldquo;Note\u0026rdquo;, and \u0026ldquo;Caution\u0026rdquo; call-outs can be found throughout the site, too. Images to match the text description. Because these instructions direct users to navigate to a feature of GitHub most of them were not familiar with, I thought including images would help them feel confident about the tasks they are being asked to perform. `}),e.add({id:5,href:"/code/exploratorydataanalysis/",title:"Exploratory Data Analysis",description:"Samples of past EDA",content:`This page contains an example of exploratory data analysis I\u0026rsquo;ve performed. Via the window below, you can explore my Jupyter notebook for this project.
Double-click the README.md in the interactive window for more information.
If the window doesn\u0026rsquo;t load properly or if you\u0026rsquo;re viewing on mobile, click here to view the source file.
For other examples of my data science projects, see this GitHub repository.
`}),e.add({id:6,href:"/docs/sampleworkproducts/",title:"Sample Work Products",description:"",content:""}),e.add({id:7,href:"/docs/sampleworkproducts/api-documentation/",title:"API Documentation",description:"A sample of my API Documentation",content:`Coming soon!
`}),e.add({id:8,href:"/docs/overview/",title:"Overview",description:"",content:""}),e.add({id:9,href:"/code/fetchweatherscript/",title:"Fetch Current Weather",description:"I like knowing the weather as soon as I get up in the morning. However, I don\u0026rsquo;t always like poking my head outside to find out, and a simple Google search doesn\u0026rsquo;t provide me all the weather details I want to know directly in the search returns page. So I wrote a Python script that gives me exactly what I want. Because I\u0026rsquo;ll occasionally travel, I\u0026rsquo;ve set it up so that, rather than simply defaulting to my primary location, I can enter the name of the city for which I want the weather details.",content:`I like knowing the weather as soon as I get up in the morning. However, I don\u0026rsquo;t always like poking my head outside to find out, and a simple Google search doesn\u0026rsquo;t provide me all the weather details I want to know directly in the search returns page. So I wrote a Python script that gives me exactly what I want. Because I\u0026rsquo;ll occasionally travel, I\u0026rsquo;ve set it up so that, rather than simply defaulting to my primary location, I can enter the name of the city for which I want the weather details.
In most cases, the API I\u0026rsquo;m using defaults to reporting in units that aren\u0026rsquo;t intuitive to me (e.g., meters/second for wind speed). I\u0026rsquo;ve added a few basic functions to convert values to units I\u0026rsquo;m more familiar with. I also can never remember what the significance of the pressure readings are, so I\u0026rsquo;ve added a function that categorizes a pressure reading as either High, Medium, or Low.
For more information on the API this script uses to retrieve current weather data, please see this page. Keys for this API can be accessed after setting up an account. For security reasons, I\u0026rsquo;ve removed my API key from the script below.
import requests def get_weather(): def kelvin_to_fahrenheit(kelvin): fahrenheit = ((kelvin - 273.15) * (9/5) + 32) return fahrenheit def millibar_to_inchesHg(millibars): inHg = millibars / 33.864 return inHg def pressure_rating(inHg): if inHg \u0026gt; 30.20: pressure = \u0026#34;High Pressure\u0026#34; return pressure elif 29.80 \u0026lt;= inHg \u0026lt;= 30.20: pressure = \u0026#34;Medium Pressure\u0026#34; return pressure else: pressure = \u0026#34;Low Pressure\u0026#34; return pressure def metersPerSecond_to_MPH(metersPerSecond): mph = metersPerSecond * 2.237 return mph api_key = \u0026#39;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\u0026#39; base_url = \u0026#39;https://api.openweathermap.org/data/2.5/weather\u0026#39; city = input(\u0026#39;Enter city name: \u0026#39;) endpoint = f\u0026#39;{base_url}?q={city}\u0026amp;appid={api_key}\u0026#39; response = requests.get(endpoint) if response.status_code == 200: data = response.json() main = data[\u0026#39;weather\u0026#39;][0][\u0026#39;main\u0026#39;] description = data[\u0026#39;weather\u0026#39;][0][\u0026#39;description\u0026#39;] temp_kelvin = data[\u0026#39;main\u0026#39;][\u0026#39;temp\u0026#39;] feels_like_kelvin = data[\u0026#39;main\u0026#39;][\u0026#39;feels_like\u0026#39;] humidity = data[\u0026#39;main\u0026#39;][\u0026#39;humidity\u0026#39;] wind_metric = data[\u0026#39;wind\u0026#39;][\u0026#39;speed\u0026#39;] wind_speed = metersPerSecond_to_MPH(wind_metric) pressure_millibar = data[\u0026#39;main\u0026#39;][\u0026#39;pressure\u0026#39;] pressure = millibar_to_inchesHg(pressure_millibar) pressure_rating = pressure_rating(pressure) temp = kelvin_to_fahrenheit(temp_kelvin) feels_like = kelvin_to_fahrenheit(feels_like_kelvin) print(f\u0026#39;Primary Weather: {main}\u0026#39;) print(f\u0026#39;Description: {description}\u0026#39;) print(f\u0026#39;Humidity: {humidity}%\u0026#39;) print(f\u0026#39;Temperature: {temp:.1f}°F\u0026#39;) print(f\u0026#39;Feels like: {feels_like:.1f}°F\u0026#39;) print(f\u0026#39;Wind speed: {wind_speed:.1f} mph\u0026#39;) print(f\u0026#39;Pressure: {pressure:.2f} inchesHg: {pressure_rating}\u0026#39;) else: error_code = response.status_code print(f\u0026#39;There was a {error_code} error\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: get_weather() Here\u0026rsquo;s a sample output for my primary location, Columbus :
Weather: Clouds Description: overcast clouds Humidity: 46% Temperature: 44.7°F Feels like: 41.6°F Wind speed: 6.6 mph Pressure: 29.94 inchesHg: Medium Pressure `}),e.add({id:10,href:"/docs/sampleworkproducts/model-documentation/",title:"Model Documentation",description:"A sample of AI/ML Model Documentation",content:`Below is an example of AI-ML documentation that I\u0026rsquo;ve created in the past. The content of this text is designed to meet expectations established in the Federal Reserve\u0026rsquo;s SR 11-7: Guidance on Model Risk Management.
Note on style: The text below relies heavily on passive voice. This is by design. Model documentation style guides recommend erring on the side of passive voice when the agent is not important or the agent is a computer script or automation process. I also do not use the second-person present tense here because this is not an instructional manual. For an example of text that more closely follows commonly used style guides (e.g., Google\u0026rsquo;s developer documentation style guide), see my Introduction to the Command Line for Technical Writers.
Disclaimer: all proprietary details have been removed and the specific model attributes outlined below have been invented.
Model Overview # Model Purpose # The purpose of Super Fantastic Superior Sample Weather Model (hereafter \u0026ldquo;Sample Weather\u0026rdquo; or \u0026ldquo;the model\u0026rdquo;) is to predict Fantastic Insurance Co.\u0026rsquo;s claim losses and loss frequencies following a catastrophic (\u0026ldquo;CAT\u0026rdquo;) wind-rain event, such as a hurricane.
Currently, when a severe weather event occurs, Fantastic Insurance actuaries produce loss and frequency estimates by reviewing past wind-rain events and applying a multiplication factor to the cost and frequencies of the previous event. This approach has proven ineffective at gauging losses and frequencies, especially when the present event differs dramatically in size and location from past wind-rain events.
The goal of the Sample Weather model is to use advanced analytics techniques to
predict losses and frequencies more accurately produce predictions more quickly than current practice allows, and continue producing predictions for the 30 days following a wind-rain event. Model Characteristics # Attribute Value Model Name Sample Weather Model Model ID No. 867-5309 Model Owner Tommy Tutone Model Client Jenny Jenny Production Date November 16, 1981 Deployment SageMaker Image ID 325389189899.dkr.ecr.us-east-2.amazonaws.com/sampleenvironment-sampleweathermodel-dev-deploy:latest Repository Details # Production Repository
https://github.com/redsoxfan0219/master/sampleweathermodel
Development Repository
https://github.com/redsoxfan0219/develop/sampleweathermodel
ETL Code
https://github.com/redsoxfan0219/ETL/sampleweathermodel
Final Training Data
s3://sample-environment/sampleweathermodel/eda/train/
Model Outputs # The model produces state-level loss and frequency estimates for the lower 48 states + the District of Columbia. The production code sends 98 model outputs to Teradata and the business partner\u0026rsquo;s S3 bucket. The output format is .csv.
Implementation Details # The model pipeline is triggered to run when a member of the catastrophe claims team registers a new catastrophic event ID within their Super Great Claims system. New IDs are typically established while storms are still forming off the Atlantic coast (i.e., before any related claims are filed).
When new claims are filed following a catastrophic event, the model runs each of the first 30 days after claim with the new CAT ID is filed. The model runs at 8:00 ET on the applicable days. The business SLA states that the model must upload predictions to the business S3 by 9:00 am ET on the applicable days.
Production Data # Data Input #1: New Claim Data # The first production data input consists of variables related to each of the new claims filed. Only claims that have the specific CAT ID flag are passed to the model. The following are the raw claim variables passed to the model:
Variable Definition Units claim_id 16-digit Unique identifier N/A req_dol Total amount filed amount in claim USD cocode US county code of claim location N/A loss_desc Text description of the loss N/A written Total written premium of the policy USD Data Input #2: Claimant History # The second data input includes the policyholder\u0026rsquo;s claim history.
Variable Definition Units claim_id 16-digit Unique identifier N/A pr_cl_fd Binary flag indicating prior claim filed on the policy in the last 12 months N/A pr_frd_nt Binary flag indicating prior fraud investigation of policyholder N/A inv_notes String of prior fraud investigator notes, if applicable; different incidents\u0026rsquo; notes separated by semicolon NA Rationale # Claim history is used to predict potential incidents of fraud. Historically, catastrophic events have invited high levels of insurance fraud.
Cleaning, Transformations, and Feature Engineering # Claimant history is filtered to limit to the prior 6 months before the first filed CAT claim.
The fraud investigator notes are stemmed and lemmatized before undergoing TF-IDF counting. The static file (.csv)outlining the most important n-grams are sourced from the production S3 bucket.
Data Input #3: NOAA Weather Data # Weather data is sourced from the National Oceanic \u0026amp; Atmospheric Administration\u0026rsquo;s High-Resolution Rapid Refresh system.
Data Input #4: Prior Day Model Objects # Past Model Objects.
Methodology, Assumptions, and Parameters # The primary modeling framework is XGBoost, whose objective function is the sum of a training loss and a regularization metric. For more information on the model equation, see here.
For each of this project\u0026rsquo;s model objects, the modeling team tuned the following hyperparameters:
alpha (range: 0-1000) colsample_bytree (range: .1-.5) eta (learning rate)(range: .1-.5) lambda(range: 0-1000) The hyperparamters values vary for each of the 30 n-day models. The list of values can be found in the xgb_hp_values.csv file, available in the project S3 bucket.
Controls # Input Controls # The following are the input controls for the production model:
Incoming variables from each source are tested against dictionaries of expected variables. If there are meaningful discrepancies, the model execution fails and an error message is logged on AWS CloudWatch. Output Controls # For each model run, the following output controls are performed:
A log of each model run is saved to the production S3 bucket When the model outputs land in the business partner S3, a confirmation email is sent to the business partner and the lead modeler. `}),e.add({id:11,href:"/code/rentext/",title:"RenText",description:`RenText is my ongoing effort to use computational tools to study Renaissance English texts, published roughly from 1470-1700.
Data Overview # The data used in this project is from the Text Creation Partnership.
The primary data currently consists of approximately 60,000 XML files, which can be accessed on the TCP\u0026rsquo;s Dropbox.
As described on this page, all of the 60K+ texts were hand-coded over the course of about 20 years. The exact nature of the EEBO source texts—digitized (often quite old and occasionally poor-quality) microfilm of original hard copies—made and continues to make optical character recognition infeasible.`,content:`RenText is my ongoing effort to use computational tools to study Renaissance English texts, published roughly from 1470-1700.
Data Overview # The data used in this project is from the Text Creation Partnership.
The primary data currently consists of approximately 60,000 XML files, which can be accessed on the TCP\u0026rsquo;s Dropbox.
As described on this page, all of the 60K+ texts were hand-coded over the course of about 20 years. The exact nature of the EEBO source texts—digitized (often quite old and occasionally poor-quality) microfilm of original hard copies—made and continues to make optical character recognition infeasible.
Data Updates # The latest updates from the TCP, available under \u0026ldquo;Phase II\u0026rdquo; on this page, indicate several thousand additional TCP texts are forthcoming. The page indicates this release was intended in 2020. However, as of October 2022, these updates have not been released.
Tools # I\u0026rsquo;m using Python for all my exploratory data analysis and cleaning. My primary libraries are pandas, re, xml.etree.ElementTree, and os.
For visualization, I am using graph modeling with neo4j.
Completed Work # Thus far, I have
Written a script that converts all .xml files to clean, human-readable .txt files Uploaded all .xml files to Amazon S3, enabling cloud-based computing Cleaned errors and anomalies in the titles\u0026rsquo; publication dates, creating a cleaned dates file that can be used for as a look-up table Written a script that returns basic metadata (author, title) for all titles for a given year Written a script that returns a random book\u0026rsquo;s information, including author, title, publication date, and sample paragraphs/lines of poetry Written a script to return instances of a word or phrase (i.e., an n-gram) for a given year Written a script to build a SQLite database with all primary metadata for all books in the TCP archive Plans # As of October 2022, my next steps include the following:
Optimize processing time in existing scripts by incorporating list comprehension and multiprocessing Publish full .txt files on S3 Building an API to expose values in SQLite database Building a website to publish returned API responses and provide download paths for full .txt files Building a Python package to simplify processing and analysis of TCP archive Building a graph database for visualizing the archive\u0026rsquo;s metadata Codebase and Supporting Resources # The code for this project can be found on this GitHub repository.
`}),e.add({id:12,href:"/docs/sampleworkproducts/package-documentation/",title:"Package Documentation",description:"Coming soon!",content:`Coming soon!
`}),e.add({id:13,href:"/docs/guides/introduction-to-the-command-line-for-technical-writers/",title:"Introduction to the Command Line for Technical Writers",description:`Are you a technical writer who wants to learn the command line? This guide is for you.
Windows, Mac, and Linux computers have slightly different command line interfaces (CLIs). Below, I include separate instructions for all three. After reading the first two sections, skip to the instructions that apply to the system you\u0026rsquo;re using.
Note for Linux users: I base my instructions on the Ubuntu distribution. You may encounter minor differences if are using a different distribution.`,content:`Are you a technical writer who wants to learn the command line? This guide is for you.
Windows, Mac, and Linux computers have slightly different command line interfaces (CLIs). Below, I include separate instructions for all three. After reading the first two sections, skip to the instructions that apply to the system you\u0026rsquo;re using.
Note for Linux users: I base my instructions on the Ubuntu distribution. You may encounter minor differences if are using a different distribution.
What Is the Command Line? # The command line is a program that makes an operating system perform actions. It works by processing textual commands written by a user. In this respect, it differs from most programs, which take their directions from a user\u0026rsquo;s mouse clicks. These other programs are known as \u0026ldquo;graphical user interfaces\u0026rdquo; (GUIs).
The command line goes by different names. Mac and Linux calls it Terminal. Windows has two CLIs, one called Command Prompt and one called PowerShell. Powershell is more versatile, and it is the Windows CLI I will use for instructions below.
Why Would a Technical Writer Want to Know the Command Line? # The biggest reason for a technical writer to learn the command line is that it\u0026rsquo;s central to docs as code, an approach to technical writing that has ballooned in popularity since the early 2010s. Docs as code involves a combination of a markup language, a static site generator, version control, and web deployment. In some cases, knowing the command line helps work with these tools. In other cases, you cannot use these tools without the command line.
Another reason to learn the command line is that it can help you do your job more efficiently. After you\u0026rsquo;ve grown accustomed to the command line, you\u0026rsquo;ll find you can do tasks like creating new files and moving files more quickly with the command line. Learning the command line also provides you most of the skills needed to learn \u0026ldquo;shell scripting,\u0026rdquo; which is a way of saving CLI commands so you can run them again. With shell scripting, you can automate boring tasks.
Finally, the command line can help you perform tasks that would be extremely tedious if you were to use a GUI tool. For example, imagine you have a few hundred documentation pages, each of which has a file name ending with three digits. Now imagine your manager asks you to add a 0 to the start of these numbers. If you had to do that manually—right-click, \u0026ldquo;Rename,\u0026rdquo; add 0, click away from the file name—it would be a mind-melting task. With the command line, you can implement this change in one line of commands.
Instructions Table of Contents # Ready to get started? Click the link that applies to your system.
Mac Instructions
Windows Instructions
Linux Instructions
Mac Instructions # Opening the Command Line # Open a Finder window. Use the search bar to search for Terminal. In the search returns, locate Terminal and double-click to open it. I recommend adding Terminal to your MacOS Dock. Click and drag the Terminal icon from your search returns and drop it in your Dock.
Understanding the Terminal Window # When first opened, the command line will display some information. This section explains what you\u0026rsquo;re seeing.
Here\u0026rsquo;s a newly opened Terminal window. Your color schema may be different.
Here\u0026rsquo;s what you\u0026rsquo;re seeing:
\u0026ldquo;Last login\u0026rdquo; gives a timestamp for the last time you opened a Terminal window. If you open a new window (⌘ + N), the timestamp will change. The next three lines relate to the \u0026ldquo;shell,\u0026rdquo; which is the program Mac uses to convert your textual commands into computer actions. Don\u0026rsquo;t worry about this for now. The last line tells you five things: (base) Macbook Pro:. This tells you the machine you\u0026rsquo;re working on. It\u0026rsquo;ll differ for your machine, unless you\u0026rsquo;re on a Macbook Pro. ~. This indicates your home directory. Terminal will start in your home directory when you open a new window. benjaminmoran. This indicates your \u0026ldquo;relative working directory.\u0026rdquo; \u0026ldquo;Relative\u0026rdquo; here means \u0026ldquo;short form,\u0026rdquo; and \u0026ldquo;working directory\u0026rdquo; indicates where the terminal is currently \u0026ldquo;looking.\u0026rdquo; More on this soon. \$. This is the prompt. It may be a % for you. It indicates where you will start entering your commands. ▮. This is the cursor. It\u0026rsquo;s likely flashing. It tells you where you\u0026rsquo;re typing. It moves as you type something. Print the Working Directory # Every new prompt line in Terminal provides a shortened form of your present working directory. Sometimes, you need to know the complete form of your present working directory. To do so, type pwd and press enter.
Contrast the first line with what the Terminal returned. The first line only lists benjaminmoran, the relative path. The absolute path printed on the next line shows that benjaminmoran is a directory sitting under the Users directory.
Listing the Contents of a Directory # When you open a Finder window in a directory, you see something like this (if you haven\u0026rsquo;t changed your settings):
When you are in a directory in Finder, you see the contents of that directory.
Unlike Finder, Terminal does not automatically list the contents of the present working directory. To see the contents of the present working directory, type ls and press Enter.
When listed, directories are printed alongside files. Directories do not have file extensions; files do.
You can also use ls to list the contents of other directories outside your present working directory. If the directory is nested beneath the present working directory, add the relative path of the directory after ls. This is called \u0026ldquo;passing an argument\u0026rdquo; to the command.
Here you see the contents of a directory that is neither my present working directory nor a subdirectory of my present working directory. Notice that I had to include a leading /. That\u0026rsquo;s part of the absolute path.
What Are Flags? # With ls you learned that \u0026ldquo;arguments\u0026rdquo; can be passed to a command. Arguments tell the computer on what object (file, directory, etc.) it should perform an action.
You can also pass \u0026ldquo;flags\u0026rdquo; to a command. Flags provide the computer additional information on options or settings you would like to apply to the command. Flags are usually signalled by a leading hyphen (e.g., -b). Some flags use two hyphens (e.g., --version).
Listing All Contents of a Directory # By default, ls returns all the visible content in a directory. However, there may be hidden content present. To see visible and hidden contents of a directory, use ls -a. The -a flag stands for \u0026ldquo;all.\u0026rdquo;
The hidden contents, which have a leading ., are printed first in the returns, followed by the visible contents.
Like many other CLI commands, ls can take both a flag and an argument in the same command. Enter ls -a \u0026lt;absolute path\u0026gt; to print all contents of a directory.
Creating a New File # Use the touch \u0026lt;file-name.extension\u0026gt; command to create new files in Terminal. You can create a new file using any extension.
Terminal doesn\u0026rsquo;t print a confirmation after a successful touch. You can verify the file was created by running ls.
Open a File # Let\u0026rsquo;s try opening the file you just created. Use the open \u0026lt;file-name.txt\u0026gt; command. You\u0026rsquo;ll know you\u0026rsquo;ve succeeded because Mac will open your empty file in your default .txt file reader (likely TextEdit).
Changing Your Working Directory # You know that Terminal defaults to the home directory when opened. What if you want to perform actions outside that directory? The easiest way is to change the working directory and perform actions afterward.
To change your working directory, enter cd \u0026lt;pathway\u0026gt;. Here, I jump down one directory from benjaminmoran into Documents. The change is reflected in the text of the next line, which shows Documents as the relative path.
You can use cd to move up a directory, too. Use cd ../.
You can move up or down more than one directory at a time. Here, I jump down two directories in one command. In the next line, I chain two ../s to return to my original directory.
Like you saw with ls, cd allows us to change directories using an absolute path regardless of our current directory.
Tab Completion # There\u0026rsquo;s a handy trick to make opening a file or changing your directory even quicker: tab completion. When writing out a directory, for example, you can press TAB after typing a few characters, and Terminal will fill in the directory. Here, I typed Doc before pressing TAB, saving me a little bit of time. When you add up those little bits of time saved, you start to realize how much more efficient this is than navigating with your mouse.
Note that, with tab completion, your computer looks for unique values based on your initial input. If it encounters two file or directory names with the same root, the terminal stops at the point that the two file names diverge. It then expects you to clarify which file or directory you intend to use. If you have two sub-directories in my present working directory, one called Document and one called Documents, and you type cd Doc and hit TAB, Terminal will fill in ument, stopping at cd Document. It expects you to either press Enter to change directory into Document or enter an s before entering into the Documents directory.
Making a New Directory # The final basic command is mkdir \u0026lt;directory-name\u0026gt;, the command you use to create a new directory. Terminal doesn\u0026rsquo;t print a confirmation after you press enter. Run an ls to confirm the directory\u0026rsquo;s creation.
Next Steps # You now know most of the CLI commands you\u0026rsquo;ll need as a technical writer. Consult this cheat sheet to explore other commands. (This cheat sheet is for Linux, but Mac\u0026rsquo;s Terminal follows Linux commands.)
If you want to take the next steps in learning the docs as code approach to documentation, check out my introductions to Git and GitHub.
Windows Instructions # Note: where applicable, I\u0026rsquo;ll be using PowerShell command aliases. Aliases are shorthand forms of their commands. Most basic PowerShell commands have an alias.
Opening the Command Line # Open the Start Menu. Search for PowerShell in the search bar. When the PowerShell icon appears, double-click to open the program. Understanding the PowerShell Window # When first opened, the command line will display some information. This section explains what you\u0026rsquo;re seeing.
Here\u0026rsquo;s a newly opened PowerShell window:
The first three lines of text include copyright information and an advertisement. You can ignore them.
The final line communicates four things:
PS — This is the PowerShell indicator. It distinguishes PowerShell from Windows’ other CLI, Command Prompt.
C:\\Users\\moranb7 — This indicates your \u0026ldquo;absolute working directory.\u0026rdquo; It tells you the full pathway where PowerShell is currently \u0026ldquo;looking.\u0026rdquo; More on this soon.
\u0026gt; — This is the prompt. It indicates where you will start entering your commands.
_ [not pictured] — This is the cursor. It\u0026rsquo;s probably flashing. It tells you where you\u0026rsquo;re typing. It moves as you type something.
Print the Working Directory # The working directory is where the command line is currently \u0026ldquo;looking.\u0026rdquo; It describes the direct where the command line can perform actions.
Unlike in other systems CLIs, PowerShell prints the complete (i.e., \u0026ldquo;absolute\u0026rdquo;) working directory with each new prompt line. Nevertheless, you can enter pwd to print the working directory.
Listing the Contents of a Directory # When you open a File Explorer window in a directory, you see something like this (if you haven\u0026rsquo;t changed your settings):
When you are in a directory in File Explorer, you see the contents of that directory.
Unlike File Explorer, PowerShell does not automatically list the contents of the present working directory. To see the contents of the present working directory, type ls and press Enter.
When listed, directories are printed alongside files. Directories do not have file extensions; files do.
You can also use ls to list the contents of other directories outside your present working directory. If the directory is nested beneath the present working directory, simply add the relative path of the directory after ls. This is called \u0026ldquo;passing an argument\u0026rdquo; to the command.
Here you see the contents of a directory that is neither my present working directory nor a subdirectory of my present working directory. Notice that I had to include a leading C:\\. That\u0026rsquo;s part of the absolute path.
What Are Flags? # With ls you learned that \u0026ldquo;arguments\u0026rdquo; can be passed to a command. Arguments tell the computer on what object (file, directory, etc.) it should perform an action.
\u0026ldquo;Flags\u0026rdquo; can also be passed to a command. Flags provide the computer additional information on options or settings you would like to apply to the command. Flags are usually signalled by a leading hyphen (e.g., -b). Some flags use two hyphens (e.g., --version).
Listing All Contents of a Directory # By default, ls returns all the visible content in a directory. However, there may be hidden content present. To see visible and hidden contents of a directory, use ls -Force. The -Force flag stands for \u0026ldquo;all.\u0026rdquo;
Note: my Windows settings have been changed to show all hidden files, so the photos below show hidden files with a simple ls.
The hidden contents, which have a leading ., appear first in the returns, followed by the visible contents.
Like many other CLI commands, ls can take both a flag and an argument in the same command. Enter ls -a \u0026lt;absolute path\u0026gt; to print all contents of a directory.
Creating a New File # Use the ni \u0026lt;file-name.extension\u0026gt; command to create new files in PowerShell. ni is the alias for the New-Item. You can write this command either way.
Using ni, you can create a new file for any extension.
When you press enter, PowerShell will print a confirmation message that includes the file permissions, creation time, and file name.
Open a File # Let\u0026rsquo;s try opening the file you just created. If you are in the same working directory as the file, type the file name (with extension) and press Enter. You know you\u0026rsquo;ve succeeded because Windows will open your empty file in your default .txt file reader (likely Notepad).
Changing Your Working Directory # You know that PowerShell defaults to the home directory when a new window is opened. What if you want to perform actions outside that directory? The easiest way is to change the working directory and perform actions afterward.
To change your working directory, enter cd .\\\u0026lt;pathway\u0026gt;. Here, I jump down one directory from moranb7 into Desktop. The change is reflected in the text of the next line, which shows Documents as the relative path.
You can use cd to move up a directory, too. Use cd ../.
You can move up or down more than one directory at a time. Here, I jump down two directories in one command. In the next line, I chain two ../s to return to my original directory.
Like we saw with ls, cd allows us to change directories using an absolute path regardless of our current directory.
Tab Completion # There\u0026rsquo;s a handy trick to make opening a file or changing your directory even quicker: tab completion. When writing out a directory, for example, you can press TAB after typing a few characters, and Terminal fills in the directory. In the screenshot above, I typed Doc before pressing TAB, saving me a little bit of time. When you add up those little bits of time saved, you start to realize how much more efficient this is than navigating with your mouse.
Note that, with tab completion, your computer looks for unique values based on your initial input. If it encounters two file or directory names with the same root, the terminal will stop at the point that the two file names diverge. It then expects you to clarify which file or directory you intend to use. If you have two sub-directories in my present working directory, one called Document and one called Documents, and you type cd Doc and hit TAB, Terminal fills in ument, stopping at cd Document. It expects you to either press Enter to change directory into Document or enter an s before entering into the Documents directory.
Making a New Directory # The final basic command is md \u0026lt;directory-name\u0026gt;, the command you use to create a new directory (the full command is mkdir). PowerShell prints a confirmation message after creating the new directory
Next Steps # You now know most of the CLI commands you need as a technical writer. Consult this cheat sheet to explore other commands.
If you want to continue learning the docs as code approach to documentation, check out my introductions to Git and GitHub.
Linux Instructions # Opening the Command Line # Click the Show Applications button. Click the Terminal icon. I recommend adding Terminal to the dash. Click and drag the Terminal icon from the Show Applications menu and drop it in the dash.
Understanding the Terminal Window # When first opened, the command line will display some information. This section explains what you\u0026rsquo;re seeing.
Here\u0026rsquo;s a newly opened Terminal window:
Here’s what you’re seeing:
benbarksdale@Ubuntu: — This string of text indicates the username and the name of the computer on which you’re working.
~ — This indicates your home directory. Terminal starts in your home directory when you open a new Terminal window. By default, your home directory is /home/\u0026lt;username\u0026gt;.
\$ — This is the prompt. It indicates where you will start entering your commands.
▮ — This is the cursor. It\u0026rsquo;s probably flashing. It tells you where you\u0026rsquo;re typing. It moves as you type something.
Print the Working Directory # Every new prompt line in Terminal provides a shortened form of your present working directory. Sometimes, you need to know the complete form of your present working directory. To do so, type pwd and press enter.
Contrast the line first line with what the Terminal returned. The first line only lists ~, the special character indicating that the relative path is the home directory. The absolute path printed on the next line shows that benjaminbarksdale is a directory sitting under the home directory.
Listing the Contents of a Directory # When you open a Files window in a directory, you see something like this (if you haven\u0026rsquo;t changed your settings):
When you are in a directory in Files, you see the contents of that directory.
Unlike Files, Terminal does not automatically list the contents of the present working directory. To see the contents of the present working directory, type ls and press Enter.
When listed, directories are printed alongside files. Directories do not have file extensions; files do. Terminal also prints files in a different color than directories.
You can also use ls to list the contents of other directories outside your present working directory. If the directory is nested beneath the present working directory, simply add the relative path of the directory after ls. This is called \u0026ldquo;passing an argument\u0026rdquo; to the command.
Here you see the contents of a directory that is neither my present working directory nor a subdirectory of my present working directory. Notice that I had to include a leading /. That\u0026rsquo;s part of the absolute path.
What Are Flags? # With ls we learned that \u0026ldquo;arguments\u0026rdquo; can be passed to a command. Arguments tell the computer on what object (file, directory, etc.) it should perform an action.
\u0026ldquo;Flags\u0026rdquo; can also be passed to a command. Flags provide the computer additional information on options or settings you would like to apply to the command. Flags are usually signalled by a leading hyphen (e.g., -b). Some flags use two hyphens (e.g., --version).
Listing All Contents of a Directory # By default, ls returns all the visible content in a directory. However, there may be hidden content present. To see visible and hidden contents of a directory, use ls -a. The -a flag stands for \u0026ldquo;all.\u0026rdquo;
The hidden contents, which have a leading ., appear alongside the visible contents.
Like many other CLI commands, ls can take both a flag and an argument in the same command. Enter ls -a \u0026lt;absolute path\u0026gt; to print all contents of a directory.
Creating a New File # Use the touch \u0026lt;file-name.extension\u0026gt; command to create new files in Terminal. You can create a new file using any extension.
Terminal doesn\u0026rsquo;t print a confirmation after a successful touch. You can verify the file was created by running ls.
Open a File # Let\u0026rsquo;s try opening the file you just created. Use the open \u0026lt;file-name.txt\u0026gt; command. You\u0026rsquo;ll know you\u0026rsquo;ve succeeded because Mac will open your empty file in your default .txt file reader (likely TextEdit).
Changing Your Working Directory # You know that Terminal defaults to the home directory when opened. What if you want to perform actions outside that directory? The easiest way is to change the working directory and perform actions afterward.
To change your working directory, enter cd \u0026lt;pathway\u0026gt;. Here, I jump down one directory from benjaminbarksdale into Desktop. The change is reflected in the text of the next line, which shows Desktop as the relative path.
You can use cd to move up a directory, too. Use cd ../.
You can move up or down more than one directory at a time. Here, I jump down two directories in one command. In the next line, I chain two ../s to return to my original directory.
Like you saw with ls, cd allows us to change directories using an absolute path regardless of our current directory.
Tab Completion # There\u0026rsquo;s a handy trick to make opening a file or changing your directory even quicker: tab completion. When writing out a directory, for example, you can press TAB after typing a few characters, and Terminal will fill in the directory. Here, I typed Doc before pressing TAB, saving me a little bit of time. When you add up those little bits of time saved, you start to realize how much more efficient this is than navigating with your mouse.
Note that, with tab completion, your computer looks for unique values based on your initial input. If it encounters two file or directory names with the same root, the terminal will stop at the point that the two file names diverge. It then expects you to clarify which file or directory you intend to use. If you have two sub-directories in my present working directory, one called Document and one called Documents, and you type cd Doc and hit TAB, Terminal fills in ument, stopping at cd Document. It expects you to either press Enter to change directory into Document or enter an s before entering into the Documents directory.
Making a New Directory # The final basic command is mkdir \u0026lt;directory-name\u0026gt;, the command you use to create a new directory. Terminal doesn\u0026rsquo;t print a confirmation after you press enter. Run an ls to confirm the directory\u0026rsquo;s creation.
Next Steps # You now know most of the CLI commands you need as a technical writer. Consult this cheat sheet to explore other commands.
If you want to continue learning the docs as code approach to documentation, check out my introductions to Git and GitHub.
`}),e.add({id:14,href:"/docs/guides/introduction-to-git-for-technical-writers/",title:"Introduction to Git for Technical Writers",description:"description",content:`Increasingly, technical writing jobs require knowledge of something called \u0026ldquo;version control.\u0026rdquo; The most popular version control system is a program called Git. It\u0026rsquo;s commonplace now for TW job posts to list Git as a required skill.
This page provides an explanation of what Git is and how to use it. I emphasize topics of interests to technical writers, especially those who write or are interested in writing developer documentation.
If you don\u0026rsquo;t want to read the explanation of what version control and Git are, you can jump to either the Git Instructions or Git Cheat Sheet.
Important: These instructions assume some familiarity with the command line. If you are new to the command line, start with my Introduction to the Command Line for Technical Writers.
What Is Version Control? # Version control is simply a way of tracking changes made to files stored within a directory on a computer. There\u0026rsquo;s a bit more to it, but that\u0026rsquo;s a good enough definition for now.
Why Should I Know Version Control? # Think about instances where you\u0026rsquo;ve worked on a document with a peer using a program like Microsoft Word. You probably ran into situations where you had to maintain different file versions. You may have exchanged several emails with attachments named Working Document_v2, Working Document_v2.1, or Working Document_Final.
If you\u0026rsquo;ve experienced situations like the one described above, you\u0026rsquo;ve already experienced the reason why version control is necessary. It\u0026rsquo;s inevitable that the system described above will break down. Someone will forget to download the latest version from their email, or someone will download the right version but forget to re-save with a different _v\u0026lt;\u0026gt; number.
The other problem with this approach is not knowing what changes were made in each file version. Maybe you can remember broadly what each version covers. But to version a document properly, you would need a computer-like memory for the details of every version—every paragraph, every word, every comma—and you would need to be able to hold those multiple versions in your head and compare them line-by-line. That\u0026rsquo;s functionally impossible.
Therefore, we need some way of systematically tracking changes from one version to the next, some way to understand what changes are associated with each version of a file.
What we need is something called a version control system.
What Is a Version Control System? # A version control system (VCS) is a set of computational tools that allow users to systematically track changes to a file or set of files within a directory. A VCS also allows for users to \u0026ldquo;roll back\u0026rdquo; to an earlier version of their file(s) if they so choose.
What is Git? # Git is a VCS. It\u0026rsquo;s by far the most popular VCS in the world.
Unlike many programs technical writers may be familiar with (MS Word, MadCap Flare, etc.), Git is a command-line tool. Therefore, it is directed by textual commands entered in the command line, not by the point-and-click direction of a mouse.
Git and GitHub: Related But Distinct # You\u0026rsquo;ll sometimes hear Git used interchangeably with GitHub. That\u0026rsquo;s wrong. Git and GitHub are separate entities and do separate things.
Git, again, is a version control system used for tracking the history and details of various file states. Git is run from the command line.
GitHub, by contrast, is a website and server used for storing Git repositories and their files. It also has a few other key functions that I won\u0026rsquo;t get into here.
What makes the distinction between Git and GitHub murkier is that they are often used in tandem. Project developers will version control their code on their local computer using Git. When they are ready, they will \u0026ldquo;push\u0026rdquo; their files and accompanying Git history to GitHub, where the files and Git history can be viewed by their co-developers.
While I won\u0026rsquo;t describe them in detail here, you should know that GitHub isn\u0026rsquo;t the only Git storage and orchestration website out there. GitLab and Bitbucket are two other examples.
So, one of the distinctions between Git and GitHub is that you can have Git without GitHub, but you can\u0026rsquo;t have GitHub without Git.
Git Instructions # If you\u0026rsquo;re ready to start using Git, this section is for you.
Git Installation # For Mac # If you\u0026rsquo;re on a Mac, you will install Git via Homebrew. To do that,
Open Terminal.
Enter /bin/bash -c \u0026quot;\$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026quot;
Wait for the downloads to finish. This can take some time.
When Homebrew has finished installing, enter brew install git.
For Windows # If you\u0026rsquo;re on a Windows machine,
Click here to download the Git Installer.
Open the installer where you\u0026rsquo;ve downloaded it (likely your Downloads folder).
Progress through the installer menu, accepting the defaults.
For Linux # If you\u0026rsquo;re on a Linux machine, the exact mechanism you\u0026rsquo;ll use will vary by your distribution.
Open Terminal. Enter the command appropriate to your distribution. If you aren\u0026rsquo;t sure of your distribution, follow the Ubuntu instructions. If you are using Fedora, RHEL, or CentOS, enter sudo dnf install git-all. If you are using Ubuntu or another Debian-based distribution, enter sudo apt install git-all Getting Started with Git # This section outlines the basic commands of Git. After reading this section, you\u0026rsquo;ll know about 80% of the functions you\u0026rsquo;ll ever use in Git.
Setting up GitHub # We\u0026rsquo;ll start our Git journey by setting up a GitHub account. If you already have a GitHub account, skip to the next section. Note that if you\u0026rsquo;re using GitHub for your job, you\u0026rsquo;ll probably use a version of GitHub called GitHub Enterprise. If that\u0026rsquo;s the case, you\u0026rsquo;ll need to set up create a separate work account through GitHub Enterprise.
Navigate to github.com. Click \u0026lsquo;Sign up\u0026rsquo; in the upper-right. Follow the prompts to set up your account. Setting Up a Git Repository # You can turn any ordinary folder into a Git repository, because Git repositories are just folders with a hidden .git file within them.
Cloning from Remote # There are two ways to set up a new Git repository:
Initiating a repository on GitHub and cloning it your local computer Initiating one on your local computer I typically do the former, so I\u0026rsquo;ll give those instructions first.
Sign in to GitHub. Click the green \u0026ldquo;New\u0026rdquo; button. On the new screen, enter a repository name. I recommend using underscores or hyphens to separate words in a repository name. Keep the accessibility option set to \u0026ldquo;Public.\u0026rdquo; Click the green \u0026ldquo;Create repository\u0026rdquo; button. On the new screen, copy your repository URL, which should appear within the \u0026ldquo;Quick Setup\u0026rdquo; pane. The URL will look something like this: https://github.com/redsoxfan0219/git-demo.git. We\u0026rsquo;re now going to add a dummy file so this repository is not empty. On the same screen, click the \u0026ldquo;README\u0026rdquo; link, which appears beneath the line with the Git URL. On the next screen, you\u0026rsquo;ll see some short text in the window pane. Scroll down and click the green \u0026ldquo;Commit new file\u0026rdquo; button\u0026quot;. Now, open a new command line window. On Windows, open PowerShell. On Mac, open Terminal. On Linux, open Terminal. cd to the directory where you want to store the local copy of your Git repository. Type git clone, then paste your the Git URL into the terminal, and hit Enter. You\u0026rsquo;ll see some stuff happen: Voilà! You\u0026rsquo;ve cloned your remote-native Git repository to your local computer. You can cd down into the repository and run an ls to see what\u0026rsquo;s there: As you can see, our local repository has the README.md file we created earlier.
Creating a Local Git Repository # We just cloned a remote copy of a Git repository to our local machine. If we don\u0026rsquo;t have a new project already set up, we can start from our local machine instead.
To start a new Git repository on your local machine,
Open a CLI window and cd to the location where you\u0026rsquo;d like to create a Git repository.
Create a new directory using mkdir \u0026lt;name-of-your-new-directory\u0026gt;.
cd into your new directory.
Enter git init -b main:
We\u0026rsquo;ve turned our new directory git-test into a Git repository, and we\u0026rsquo;ve given it a branch (via that -b flag) named main. More on branches in just a bit.
Staging Your Changes # After you\u0026rsquo;ve created a new Git repository via git init, you need to add some content to the repository before making your first \u0026ldquo;commit.\u0026rdquo; A commit essentially saves a snapshot of your Git repository at a given moment in time. Here, I\u0026rsquo;m going to use the Mac Terminal\u0026rsquo;s touch \u0026lt;file-name.extension\u0026gt; command to create a new dummy file (PowerShell users can enter the New-Item \u0026lt;file-name.extension\u0026gt; command):
ls confirms the new file has been created.
Now, we\u0026rsquo;re (almost) ready to commit our work. Unlike in other programs you may be used to, Git \u0026ldquo;stages\u0026rdquo; changes before committing them. There\u0026rsquo;s a good reason for that, but I won\u0026rsquo;t get into it here.
To stage your changes, simply run
git add . The . is a shorthand which means \u0026ldquo;all\u0026rdquo;. Here, that means stage all files that have been changed. If you\u0026rsquo;d prefer, you can also stage files individually, like so:
git add README.md The terminal won\u0026rsquo;t print a confirmation after you stage your content. However, you can run the git status command to check if files have been staged. Files listed in green have been staged; files listed in red have not. Here are the results of running git status before and staging the file:
Committing Your Contents # Finally, we\u0026rsquo;re ready to commit our changes.
To commit your changes, run git commit -m \u0026quot;some message\u0026quot;, replacing \u0026ldquo;some message\u0026rdquo; with a meaningful description of the changes reflected in this commit. Don\u0026rsquo;t skimp on the message! You may need to find this commit later, and the message will help you know what this commit captures.
After you press Enter, Git will display a summary of your committed changes.
Run git log to see your most recent changes, beginning with your latest commit. I\u0026rsquo;ve made a few additional commits here for demonstration purposes.
That long string of letters and numbers is the \u0026ldquo;commit hash.\u0026rdquo; Each commit hash is unique. The commit hash is the identifier that we\u0026rsquo;ll use if we ever need to \u0026ldquo;roll back\u0026rdquo; our contents to match the state of the repository reflected in the commit hash.
Moving Your Local Git Repository to GitHub # If you\u0026rsquo;ve initiated your Git repository on your local machine and you\u0026rsquo;ve made at least one commit, you\u0026rsquo;re ready to connect your local Git repository to a remote Git storage system like GitHub.
To do so,
Navigate to GitHub and sign-in to your account if necessary.
Click the + in the upper-right corner, and select \u0026ldquo;New repository\u0026rdquo; from the dropdown.
Add a repository name and click the green \u0026ldquo;Create repository\u0026rdquo; button. While not strictly required, it is a very good idea to match this name to the name of your local Git repository.
On the next page, copy the Git URL.
Switch over to your CLI window.
If necessary, cd into your local Git repository.
Type git remote add origin, paste your GitHub URL, and press Enter.
Verify the connection by entering git remote -v. Both the fetch and push URLs should match the URL you just entered.
Git Branches # The last major thing I\u0026rsquo;ll discuss in this introduction is what branching is and how it will affect your Git use.
Branches are features of distributed version control that allow you to store multiple versions of the same Git repository. Having multiple branches allows each developer to work on version of the codebase (or docbase) before merging their contributions to another branch designated by a team as the primary branch. Nor are you restricted to one branch per person. You might want to perform your testing on a test branch, draft your documentation on a docs-draft branch, and your save your polished code on a feature branch. The names can be whatever you want. However, a development team may have a \u0026ldquo;branching strategy\u0026rdquo; that guides how branches are named and how they interact with one another.
Adding a New Branch # A few steps ago, we created our first branch when we initiated a Git repository. The command we used was git init -b main. You will use a different Git command for creating subsequent new branches.
There are a few ways of creating new branches, but the one I like most is git checkout -b \u0026lt;branch-name\u0026gt;. This step combines two steps: it creates a new branch and changes your working branch to the new branch. You can confirm which branch you are on my entering git branch. The following image demonstrates what git branch shows before and after running git checkout -b \u0026lt;branch-name\u0026gt;.
While Git prints a message after running git checkout, you should run git branch regularly to ensure you\u0026rsquo;re working on your intended branch.
Switching to an Existing Local Branch # While git checkout -b \u0026lt;branch\u0026gt; allows you to switch to a new branch, sometimes you\u0026rsquo;ll need to switch to a different branch you previously created. To do so, use git switch \u0026lt;existing-branch-name\u0026gt;.
If you can\u0026rsquo;t remember the existing branch name, run git branch to double-check the existing local branches.
Pushing a Local Branch to Remote # It\u0026rsquo;s a good practice to send your local branch changes to the remote at least periodically. After staging and committing your changes, all you need to do is run git push.
Getting All Remote Branches on Your Local Repository # Especially when working with collaborators, you will find that your remote repository eventually contains more branches than your local repository.
Getting all the remote branches and their updates is a bit complicated. To make things simple, copy the commands below:
git branch -r | grep -v \u0026#39;\\-\u0026gt;\u0026#39; | sed \u0026#34;s,\\x1B\\[[0-9;]*[a-zA-Z],,g\u0026#34; | while read remote; do git branch --track \u0026#34;\${remote#origin/}\u0026#34; \u0026#34;\$remote\u0026#34;; done git fetch --all git pull --all Git Cheat Sheet # This section consolidates all the commands I\u0026rsquo;ve described above. It adds a few others, too.
git add \u0026lt;. or filename.extension\u0026gt;
Stages your change(s) prior to a commit. git commit -m \u0026lt;\u0026quot;commit message\u0026quot;\u0026gt;
Commits your changes. -m flag + \u0026quot;commit message\u0026quot; used to explain what the commit involves. git remote add origin \u0026lt;GitHub URL\u0026gt;
Establishes remote Git repository connection for repositories created locally. git remote -v
Prints the fetch and push URLs of the remote Git repository (e.g., GitHub). git fetch
Retrieves the latest metadata from the remote branch. Does NOT change the codebase on the local Git repository. git pull
Retrieves the latest metadata from the remote branch AND updates the codebase in the local working branch. git push
When a connection to remote has been established and at least one new commit has been made, transmits commit contents and metadata to remote Git repository. git checkout -b \u0026lt;branch-name\u0026gt;
Creates a new branch and switches the working branch to it. git switch \u0026lt;branch-name\u0026gt;
Changes the working branch. git branch
Prints all local branches and identifies the working branch with a *. git branch -vv
Prints all local branches alongside each branch\u0026rsquo;s most recent commit details (shortform commit hash and commit message). Identifies the working branch with a *. git branch -a
Prints all local AND remote branches. Identifies the local working branch with a *. git log
Prints details for the last three commits, including the commit hash, commit author, and the timestamp of the commit. Next Steps # This page covers most of what you\u0026rsquo;ll ever need from Git. Occasionally, you may need to use Git in more advanced ways. For those situations, I recommend downloading and consulting Scott Chacon and Ben Staub\u0026rsquo;s Pro Git, which you can download for free here.
If you\u0026rsquo;d like to take the next steps in learning the docs as code approach to documentation, check out the next article in the series, Introduction to GitHub for Technical Writers.
`}),e.add({id:15,href:"/docs/guides/introduction-to-github-for-technical-writers/",title:"Introduction to GitHub for Technical Writers",description:`Since its founding in 2008, GitHub has grown to become a vital tool for most software and AI/ML teams. With the emergence of the docs as code approach to technical writing, GitHub increasingly is a part of the technical writer\u0026rsquo;s toolkit, too.
This page provides an explanation of what GitHub is, what its most important features are, and how to use it. I emphasize topics of interests to technical writers, especially those who write or are interested in writing developer documentation.`,content:`Since its founding in 2008, GitHub has grown to become a vital tool for most software and AI/ML teams. With the emergence of the docs as code approach to technical writing, GitHub increasingly is a part of the technical writer\u0026rsquo;s toolkit, too.
This page provides an explanation of what GitHub is, what its most important features are, and how to use it. I emphasize topics of interests to technical writers, especially those who write or are interested in writing developer documentation.
What Is GitHub? # GitHub is a website and server used primarily by software developers to store projects and version their code with Git. It is the world\u0026rsquo;s most popular platform for storing code.
(If you don\u0026rsquo;t know what Git is, check out my Introduction to Git for Technical Writers.)
In addition to providing a place to store their code, GitHub enables developers to collaborate with one another easily.
Why Should I Know about GitHub? # GitHub is often a key part of the docs as code approach to technical writing.
Under this approach, documentation is written in a lightweight markup language (e.g., Markdown or something similar); version controlled with Git; stored, edited, and revised on GitHub; rendered into HTML using a static site generator (e.g., Jekyll, Sphinx, etc.), and finally hosted on GitHub Pages, Read the Docs, Netlify, or another platform.
There are there are alternative tools (e.g., GitLab) that can take the place of GitHub in the docs as code approach. But the popularity of GitHub is the best reason to learn it over its competitors.
Put simply, if you take a TW job where the docs as code approach is used, chances are good you\u0026rsquo;ll be using GitHub every day.
GitHub vs. Git # You\u0026rsquo;ll sometimes hear GitHub used interchangeably with Git. That\u0026rsquo;s wrong. Git and GitHub are separate entities and do separate things.
Git is a version control system used for tracking the history and details of various file states. Git is run from the command line.
GitHub, by contrast, is a website and server that uses Git and stores Git repositories and their files. It also has a few other key functions that I\u0026rsquo;ll explain at greater length below.
What makes the distinction between Git and GitHub murkier is that they are often used in tandem. Project developers will version control their code on their local computer using Git. When they are ready, they will \u0026ldquo;push\u0026rdquo; their files and accompanying Git history to GitHub, where the files and Git history can be viewed by their co-developers.
GitHub Instructions # Ready to start using GitHub? This section is for you.
I\u0026rsquo;ll start by walking you through how to set up and configure your GitHub account. Then, I\u0026rsquo;ll outline four key skills you\u0026rsquo;ll need to know to use GitHub effectively as a technical writer:
Setting up and navigating repositories Setting up and working with branches Working with Issues Opening and using Pull Requests If you learn these topics, you\u0026rsquo;ll know almost everything you\u0026rsquo;ll need to use GitHub as a technical writer.
Getting Started with GitHub # Creating Your Account # While you don\u0026rsquo;t need a GitHub account to access public GitHub repositories like this one, but you will need one to contribute to a repository. So let\u0026rsquo;s start by setting up a new GitHub account.
To set up a new account,
Navigate to github.com and click \u0026ldquo;Sign Up.\u0026rdquo; Follow the prompts to set your email, password, and username. Retrieve the verification code from your email and enter it. Ignore for now the various add-ons and offerings. Just click next. Select the \u0026ldquo;free\u0026rdquo; option when asked if you\u0026rsquo;d like the free or premium option. The Main Dashboard # The first screen you\u0026rsquo;ll see after logging in is your user dashboard. It looks like this:
`}),e.add({id:16,href:"/docs/guides/github-actions/",title:"GitHub Actions",description:`Introduction to State Site Generators # The purpose of static site generators is two-fold:
To allow uses to write in a simple, lightweight markup language (typically Markdown or reStructuredText) To render that simple markup text into attractive, stylized HTML files The beauty of static site generators is that, after some initial setup, the process is reduced to a single command line statement, like the one I use to build the HTML for this site on my local machine:`,content:`Introduction to State Site Generators # The purpose of static site generators is two-fold:
To allow uses to write in a simple, lightweight markup language (typically Markdown or reStructuredText) To render that simple markup text into attractive, stylized HTML files The beauty of static site generators is that, after some initial setup, the process is reduced to a single command line statement, like the one I use to build the HTML for this site on my local machine:
sphinx-build -b html docs/ docs/_build As you can see, this statement only requires one to specify the source for the markup files, the destination of the outputs, and the format of those outputs.
Shortcoming of Static Site Generators # While convenient in many ways, static site generators are not perfect on their own.
For example, imagine you are building a documentation template that will be used by a large team. You want everyone to write in the Markdown or reStructuredText files you\u0026rsquo;ve built, and you want everyone to render those files into HTML when finished. Finally, you want them to host those HTML files as a GitHub Pages site that is attached to their code repository.
You\u0026rsquo;re not worried about the team writing in Markdown. But you don\u0026rsquo;t want to worry about everyone rendering the files into HTML, a process many of them may be unfamiliar with. What if they don\u0026rsquo;t remember to add the rendered files into the correct output destination? What if they forget to add that required -b flag?
As a technical writer, you want to support your team, but you also want to make your life easier, too. Every time you ask others to perform manual processes, you\u0026rsquo;re introducing a risk that others will come to you for help with those processes. How can you minimize the chances of that happening, allowing you to focus on more important tasks?
GitHub Actions, a CI/CD Tool # Generally, it\u0026rsquo;s a good idea to reduce the number of times users have to do the same action manually. Automation is almost always preferable. With the HTML build process, we have an excellent candidate for automation. But how do we do it?
The answer is GitHub Actions.
Introduced a few years back, GitHub Actions is a mechanism that can be used to automate workflows. It\u0026rsquo;s grown to become a key function for many teams\u0026rsquo; continuous integration, continuous deployment (CI/CD).
Under the hood, GitHub Actions works by receiving directions you write in a .yml file, which you post within your code repository under a ./.github/workflows subdirectory. This file provides instructions for instantiating and directing a virtual machine.
Using a GitHub Action in a Docs-as-Code Deployment # For our purposes, GitHub Actions is valuable because it can be used to automate the HTML build process and commit that content to your GitHub repository.
The best part? It\u0026rsquo;s super simple.
This page was built using a GitHub Action I put together in 20 minutes. You can find that .yml file here. I\u0026rsquo;ve also reproduced the contents of that file below.
Explanation of the GitHub Action .YML # Below are the contents of the file I wrote for my GitHub Action. I provide a walkthrough of the contents further down.
name: docs_pages_render on: push: branches: - main jobs: build_docs_job: runs-on: ubuntu-latest env: GITHUB_PAT: \$ {{ secrets.GITHUB_TOKEN }} steps: - name: Check out the main branch to the VM uses: actions/checkout@v2.3.4 - name: Set up Python in the VM uses: actions/setup-python@v2.2.1 with: python-version: 3.9 - name: Install dependencies run: | python -m pip install sphinx python -m pip install furo python -m pip install myst-parser python -m pip install sphinx_togglebutton python -m pip install sphinx-copybutton - name: Render HTML run: sphinx-build -b html docs/ docs/_build - name: Set up temporary repository and commit to HTML files run: | cd docs/_build git init touch .nojekyll git add . git config --local user.email \u0026#34;action@github.com\u0026#34; git config --local user.name \u0026#34;GitHub Action\u0026#34; git commit -m \u0026#39;Deploy rendered HTML\u0026#39; - name: Push rendered HTML to destination branch uses: ad-m/github-push-action@v0.6.0 with: github_token: \${{ secrets.GITHUB_TOKEN }} branch: gh-pages force: true directory: ./docs/_build Here are the steps performed as a result of the .yml file above:
When the repository\u0026rsquo;s event monitor detects a git push to the main branch, GitHub stands up a Linux (with Ubuntu distribution) virtual machine (VM).
The VM checks out the main branch of the repository.
The VM sets up Python v3.9.
The VM downloads (via pip) all necessary dependencies to build the HTML files.These dependencies are all outlined in the repository\u0026rsquo;s docs/conf.py file.
The VM renders the HTML from the reStructuredText and Markdown files.
The VM changes directory into the _build output directory.
The VM initiates a Git repository within the _build output directory.
The VM stages and commits the newly rendered HTML files, using a default git commit message.
Finally, the VM pushes the committed files back to my repository and deploys the HTML to the gh-pages branch.
And voilà! We have our rendered and deployed HTML. Click here to see the GitHub Pages site that I built using this CI/CD pipeline.
`}),e.add({id:17,href:"/docs/",title:"Docs",description:"",content:""}),e.add({id:18,href:"/graphics/",title:"Graphics",description:"",content:""}),e.add({id:19,href:"/graphics/processflow/",title:"Process Flow",description:"A sample process flow map",content:`Process flow maps are vital to technical documentation. Whether used to describe business processes or data product architecture, they help everyone get on the same page by visualizing the abstract.
There are several software programs out there that can be used to create process flow maps. I\u0026rsquo;m most familiar with Microsoft Visio, a program in which I\u0026rsquo;ve developed custom templates and custom stencils. I am also familiar with Miro and FigJam. For the latter, I\u0026rsquo;ve built basic plugins using TypeScript via the Figma Plugin API.
In my past work, I have used process flow maps primarily to visualize how AI/ML models are structured. With the help of my process flow maps, confused business partners and those outside the dev project team can begin to understand how a data product is functioning.
Below is a generic example of the kind of diagram I complete regularly in Visio. As you can see, this diagram visualizes how data comes into a model, how it is transformed, how it is processed, and what happens to outputs after they are produced.
`}),e.add({id:20,href:"/graphics/linkedinpost/",title:"LinkedIn Post",description:`Occasionally, I\u0026rsquo;ve been asked to produce graphics for social media posts on sites like LinkedIn. My go-to software for such requests is Adobe Illustrator.
One recent request was for a graphic celebrating my department\u0026rsquo;s cohort of incoming interns. I took pains to ensure consistency with enterprise-wide brand standards and recent section-wide guidance on social media engagement. Here\u0026rsquo;s what I came up with:`,content:`Occasionally, I\u0026rsquo;ve been asked to produce graphics for social media posts on sites like LinkedIn. My go-to software for such requests is Adobe Illustrator.
One recent request was for a graphic celebrating my department\u0026rsquo;s cohort of incoming interns. I took pains to ensure consistency with enterprise-wide brand standards and recent section-wide guidance on social media engagement. Here\u0026rsquo;s what I came up with:
`}),e.add({id:21,href:"/personal/",title:"Personal",description:"Before I started working in technical writing, I was an academic specializing in Renaissance English literature. I got my PhD at Ohio State. If you\u0026rsquo;re having trouble sleeping, you can read my dissertation. As you might expect from someone who studied literature, I like to read a lot. I still read Renaissance literature occasionally — Edmund Spenser and John Milton are still among my favorites. I\u0026rsquo;ve also come to really enjoy the work of Marcel Proust.",content:`Before I started working in technical writing, I was an academic specializing in Renaissance English literature. I got my PhD at Ohio State. If you\u0026rsquo;re having trouble sleeping, you can read my dissertation. As you might expect from someone who studied literature, I like to read a lot. I still read Renaissance literature occasionally — Edmund Spenser and John Milton are still among my favorites. I\u0026rsquo;ve also come to really enjoy the work of Marcel Proust. Beyond \u0026ldquo;highbrow literature\u0026rdquo;, I read widely in sociology, politics, natural history, music, and popular science. This, this, and this are a few books I read recently and enjoyed. During the pandemic, a friend introduced me to fly fishing, which has become a big part of my life. I try to spend at least a couple days a month on a river — even in the winter. Here\u0026rsquo;s a photo of me on the Au Sable River in Michigan in December. It was a very brisk 29°F with a wind chill of 6°! And, alas, no fish that day to offset the bitter cold.
I also like spending time with my family. My wife Nadia and I have been married for about five years. We recently bought kayaks and we\u0026rsquo;ve been enjoying taking them out on the local waterways.
We also have a pug named Maple, who likes to think of herself as a guard dog. In truth, she\u0026rsquo;s better at sleeping than she is at scaring away the mailman and local joggers. Occasionally, she\u0026rsquo;ll take a break from her busy life to pose for a photo.
`}),e.add({id:22,href:"/",title:"Hi, I'm Ben Barksdale.",description:"Technical Writing, Documentation, Information Architecture",content:""}),e.add({id:23,href:"/resume/",title:"Resume",description:"The formal record of my past work and education",content:`Experience # Senior Documentation Engineer
Nationwide Mutual Insurance Company, Enterprise Analytics Office
2019—Present
Provide end-to-end documentation services for a variety of technical audiences, including data scientists, executives, internal auditors, and state regulators Lead change management and training efforts on topics such as CI/CD and MLOps Read and write sample code (Python, R, and SQL) for documentation purposes Collaborate with software engineers, data scientists, data engineers, and other subject matter experts within Agile framework Supervise the work of three junior documentation engineers Develop and curate technical content—e.g., sample code, training videos, instructional manuals— for enterprise data scientists on topics such as Docker, Kubernetes, Kubeflow, and AWS products (S3, SageMaker, CloudWatch) Edit whitepapers on technical subjects such as natural language processing, gradient boosting machines, exploratory data analysis best practices, etc. Use scripting languages (Python, R, PowerShell) to develop tools to automate tasks such as environment configuration and documentation processes Analyst Intern
US Government Accountability Office, Contracting and National Security Acquisitions
2018—2019
Audited US Department of Defense major defense acquisition programs Wrote and edited technical reports on military spending and acquisition practices Graduate Teaching Associate
The Ohio State University, Department of English
2016—2018
Taught college writing and literature classes to more than 300 students Facilitated content in-person and online Education # PhD, English — The Ohio State University (2020)
MA, English — The University of Alabama (2015)
BA, English — Western Michigan University (2013)
Skills \u0026amp; Technology # Advanced Proficiency # Writing
Technical Writing Developer Documentation Model Documentation Editing Copyediting Proofreading Technology
GitHub SharePoint MS Office Markdown Other
Developing content for technical audiences Process Management Model Risk Management Project Management Learning Development Intermediate Proficiency # Writing
API Documentation Package / SDK Documentation Docs as Code Technology
Python reStructuredText R HTML/CSS Bash/PowerShell/Command Line Git Illustrator Other
Corporate Training Instructional Design Model Development Lifecycle Basic Proficiency # Technology
SQL Go Julia GIMP Other
Qualitative Research `}),e.add({id:24,href:"/contributors/",title:"Contributors",description:"",content:""}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()